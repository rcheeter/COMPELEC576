{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **mnist_convnet**\n",
    "- ELEC 576 HW 1\n",
    "- Robert Heeter\n",
    "- 4 October 2023\n",
    "\n",
    "## **Structure**:\n",
    "1) Set PyTorch metadata\n",
    "    - Seed\n",
    "    - TensorFlow output\n",
    "    - Whether to transfer to gpu (cuda)\n",
    "2) Import data\n",
    "    - Download data\n",
    "    - Create data loaders with batchsize, transforms, scaling\n",
    "3) Define model architecture, loss, and optimizer\n",
    "4) Define test and training loops\n",
    "    - Train:\n",
    "        - Get next batch\n",
    "        - Forward pass through model-\n",
    "        - Calculate loss\n",
    "        - Backward pass from loss (calculates the gradient for each parameter)\n",
    "        - Optimizer: performs weight updates\n",
    "5) Perform training over multiple epochs\n",
    "    - Each epoch:\n",
    "        - Call train loop\n",
    "        - Call test loop\n",
    "\n",
    "## **Acknowledgements**:\n",
    "- https://colab.research.google.com/drive/1i9KpbQyFU4zfq8zLLns8a2Kd8PRMGsaZ\n",
    "- https://github.com/motokimura/pytorch_tensorboard/blob/master/main.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rch/opt/anaconda3/lib/python3.9/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: 'dlopen(/Users/rch/opt/anaconda3/lib/python3.9/site-packages/torchvision/image.so, 0x0006): Symbol not found: __ZN3c106detail19maybe_wrap_dim_slowIxEET_S2_S2_b\n",
      "  Referenced from: /Users/rch/opt/anaconda3/lib/python3.9/site-packages/torchvision/image.so\n",
      "  Expected in: /Users/rch/opt/anaconda3/lib/python3.9/site-packages/torch/lib/libc10.dylib'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?\n",
      "  warn(\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.autograd import Variable\n",
    "import numpy as np\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from datetime import datetime\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-05 01:07:41.510256: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "# 1. Set PyTorch metadata\n",
    "\n",
    "batch_size = 64\n",
    "test_batch_size = 1000\n",
    "epochs = 1\n",
    "lr = 0.01\n",
    "try_cuda = True\n",
    "seed = 1000\n",
    "logging_interval = 10 # how many batches to wait before logging\n",
    "logging_dir = None\n",
    "\n",
    "# setting up the logging\n",
    "log_dir = os.path.join(os.getcwd(),'mnist_convnet_log', datetime.now().strftime('%b%d_%H-%M-%S'))\n",
    "writer = SummaryWriter(log_dir=log_dir)\n",
    "\n",
    "# deciding whether to send to the cpu or not if available\n",
    "if torch.cuda.is_available() and try_cuda:\n",
    "    cuda = True\n",
    "    torch.cuda.mnaual_seed(seed)\n",
    "else:\n",
    "    cuda = False\n",
    "    torch.manual_seed(seed)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Import data\n",
    "\n",
    "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.01307,), (0.3081,))])\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(datasets.MNIST('data', train=True, download=True, transform=transform),\n",
    "                                           batch_size=batch_size,\n",
    "                                           shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(datasets.MNIST('data', train=False, download=True, transform=transform),\n",
    "                                          batch_size=test_batch_size,\n",
    "                                          shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Defining model architecture, loss, and optimizer\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 10, kernel_size=5)\n",
    "        self.conv2 = nn.Conv2d(10, 20, kernel_size=5)\n",
    "        self.conv2_drop = nn.Dropout2d()\n",
    "        self.fc1 = nn.Linear(320, 50)\n",
    "        self.fc2 = nn.Linear(50, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x = F.relu(F.max_pool2d(self.conv1(x), 2))\n",
    "        # x = F.relu(F.max_pool2d(self.conv2_drop(self.conv2(x)), 2))\n",
    "        # x = x.view(-1, 320) # (batch_size, units)\n",
    "        # x = F.relu(self.fc1(x))\n",
    "        # x = F.dropout(x, training=self.training)\n",
    "        # x = self.fc2(x)\n",
    "        # x = F.softmax(x, dim=1)\n",
    "\n",
    "        x = self.conv1(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        x = self.conv2(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        x = x.view(-1, 320)\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.dropout(x, p=0.5)\n",
    "        x = self.fc2(x)\n",
    "        x = F.softmax(x, dim=1)\n",
    "\n",
    "\n",
    "        return x\n",
    "\n",
    "model = Net()\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (3020366117.py, line 31)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[8], line 31\u001b[0;36m\u001b[0m\n\u001b[0;31m    log model parameters to TensorBoard at every epoch\u001b[0m\n\u001b[0m        ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "# 4. Define test and training loops\n",
    "\n",
    "eps=1e-13\n",
    "\n",
    "def train(epoch):\n",
    "    model.train()\n",
    "\n",
    "    # criterion = nn.CrossEntropyLoss()\n",
    "    criterion = nn.NLLLoss(size_average=False)\n",
    "\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        if cuda:\n",
    "            data, target = data.cuda(), target.cuda()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data) # forward pass\n",
    "        loss = criterion(torch.log(output+eps), target) # = sum_k(-t_k * log(y_k))\n",
    "        loss.backward() # backward pass\n",
    "        optimizer.step()\n",
    "\n",
    "        if batch_idx % logging_interval == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), loss.data)\n",
    "            )\n",
    "\n",
    "            # log train/loss to TensorBoard at every iteration\n",
    "            n_iter = (epoch - 1) * len(train_loader) + batch_idx + 1\n",
    "            writer.add_scalar('train/loss', loss.data, n_iter)\n",
    "\n",
    "#     log model parameters to TensorBoard at every epoch\n",
    "    for name, param in model.named_parameters():\n",
    "        layer, attr = os.path.splitext(name)\n",
    "        attr = attr[1:]\n",
    "        writer.add_histogram('{}/{}'.format(layer, attr), param.clone().cpu().data.numpy(), n_iter)\n",
    "\n",
    "def test(epoch):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    \n",
    "    # criterion = nn.CrossEntropyLoss()\n",
    "    # criterion = nn.CrossEntropyLoss(size_average = False)\n",
    "    criterion = nn.NLLLoss(size_average = False)\n",
    "\n",
    "    for data, target in test_loader:\n",
    "        if cuda:\n",
    "            data, target = data.cuda(), target.cuda()\n",
    "\n",
    "        output = model(data)\n",
    "\n",
    "        test_loss += criterion(torch.log(output+eps), target,).item() # sum up batch loss (later, averaged over all test samples)\n",
    "        pred = output.data.max(1, keepdim=True)[1] # get the index of the max log-probability\n",
    "        correct += pred.eq(target.data.view_as(pred)).cpu().sum()\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    test_accuracy = 100. * correct / len(test_loader.dataset)\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.2f}%)\\n'.format(\n",
    "        test_loss, correct, len(test_loader.dataset), test_accuracy)\n",
    "    )\n",
    "\n",
    "    # log test/loss and test/accuracy to TensorBoard at every epoch\n",
    "    n_iter = epoch * len(train_loader)\n",
    "    writer.add_scalar('test/loss', test_loss, n_iter)\n",
    "    writer.add_scalar('test/accuracy', test_accuracy, n_iter)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rch/opt/anaconda3/lib/python3.9/site-packages/torch/nn/_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.\n",
      "  warnings.warn(warning.format(ret))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: 22.950384\n",
      "Train Epoch: 1 [640/60000 (1%)]\tLoss: 33.289719\n",
      "Train Epoch: 1 [1280/60000 (2%)]\tLoss: 16.805220\n",
      "Train Epoch: 1 [1920/60000 (3%)]\tLoss: 13.088162\n",
      "Train Epoch: 1 [2560/60000 (4%)]\tLoss: 31.054844\n",
      "Train Epoch: 1 [3200/60000 (5%)]\tLoss: 7.909535\n",
      "Train Epoch: 1 [3840/60000 (6%)]\tLoss: 9.073887\n",
      "Train Epoch: 1 [4480/60000 (7%)]\tLoss: 10.007343\n",
      "Train Epoch: 1 [5120/60000 (9%)]\tLoss: 11.657882\n",
      "Train Epoch: 1 [5760/60000 (10%)]\tLoss: 27.555960\n",
      "Train Epoch: 1 [6400/60000 (11%)]\tLoss: 13.942442\n",
      "Train Epoch: 1 [7040/60000 (12%)]\tLoss: 7.732531\n",
      "Train Epoch: 1 [7680/60000 (13%)]\tLoss: 7.687317\n",
      "Train Epoch: 1 [8320/60000 (14%)]\tLoss: 22.527254\n",
      "Train Epoch: 1 [8960/60000 (15%)]\tLoss: 16.002359\n",
      "Train Epoch: 1 [9600/60000 (16%)]\tLoss: 22.444063\n",
      "Train Epoch: 1 [10240/60000 (17%)]\tLoss: 29.177097\n",
      "Train Epoch: 1 [10880/60000 (18%)]\tLoss: 26.238842\n",
      "Train Epoch: 1 [11520/60000 (19%)]\tLoss: 11.567828\n",
      "Train Epoch: 1 [12160/60000 (20%)]\tLoss: 2.521162\n",
      "Train Epoch: 1 [12800/60000 (21%)]\tLoss: 7.144764\n",
      "Train Epoch: 1 [13440/60000 (22%)]\tLoss: 10.854490\n",
      "Train Epoch: 1 [14080/60000 (23%)]\tLoss: 12.547478\n",
      "Train Epoch: 1 [14720/60000 (25%)]\tLoss: 7.146852\n",
      "Train Epoch: 1 [15360/60000 (26%)]\tLoss: 17.599237\n",
      "Train Epoch: 1 [16000/60000 (27%)]\tLoss: 15.615850\n",
      "Train Epoch: 1 [16640/60000 (28%)]\tLoss: 11.991643\n",
      "Train Epoch: 1 [17280/60000 (29%)]\tLoss: 13.326151\n",
      "Train Epoch: 1 [17920/60000 (30%)]\tLoss: 23.508293\n",
      "Train Epoch: 1 [18560/60000 (31%)]\tLoss: 32.110165\n",
      "Train Epoch: 1 [19200/60000 (32%)]\tLoss: 13.374595\n",
      "Train Epoch: 1 [19840/60000 (33%)]\tLoss: 11.223846\n",
      "Train Epoch: 1 [20480/60000 (34%)]\tLoss: 12.475903\n",
      "Train Epoch: 1 [21120/60000 (35%)]\tLoss: 18.743235\n",
      "Train Epoch: 1 [21760/60000 (36%)]\tLoss: 19.340813\n",
      "Train Epoch: 1 [22400/60000 (37%)]\tLoss: 23.075649\n",
      "Train Epoch: 1 [23040/60000 (38%)]\tLoss: 12.355849\n",
      "Train Epoch: 1 [23680/60000 (39%)]\tLoss: 23.058193\n",
      "Train Epoch: 1 [24320/60000 (41%)]\tLoss: 11.943332\n",
      "Train Epoch: 1 [24960/60000 (42%)]\tLoss: 24.120026\n",
      "Train Epoch: 1 [25600/60000 (43%)]\tLoss: 26.227367\n",
      "Train Epoch: 1 [26240/60000 (44%)]\tLoss: 24.812790\n",
      "Train Epoch: 1 [26880/60000 (45%)]\tLoss: 7.169875\n",
      "Train Epoch: 1 [27520/60000 (46%)]\tLoss: 14.526513\n",
      "Train Epoch: 1 [28160/60000 (47%)]\tLoss: 29.294464\n",
      "Train Epoch: 1 [28800/60000 (48%)]\tLoss: 22.692366\n",
      "Train Epoch: 1 [29440/60000 (49%)]\tLoss: 24.894695\n",
      "Train Epoch: 1 [30080/60000 (50%)]\tLoss: 34.058266\n",
      "Train Epoch: 1 [30720/60000 (51%)]\tLoss: 34.304062\n",
      "Train Epoch: 1 [31360/60000 (52%)]\tLoss: 11.712961\n",
      "Train Epoch: 1 [32000/60000 (53%)]\tLoss: 23.678730\n",
      "Train Epoch: 1 [32640/60000 (54%)]\tLoss: 10.711662\n",
      "Train Epoch: 1 [33280/60000 (55%)]\tLoss: 19.656971\n",
      "Train Epoch: 1 [33920/60000 (57%)]\tLoss: 15.681749\n",
      "Train Epoch: 1 [34560/60000 (58%)]\tLoss: 7.721809\n",
      "Train Epoch: 1 [35200/60000 (59%)]\tLoss: 5.473527\n",
      "Train Epoch: 1 [35840/60000 (60%)]\tLoss: 22.136562\n",
      "Train Epoch: 1 [36480/60000 (61%)]\tLoss: 10.572528\n",
      "Train Epoch: 1 [37120/60000 (62%)]\tLoss: 5.597843\n",
      "Train Epoch: 1 [37760/60000 (63%)]\tLoss: 45.481361\n",
      "Train Epoch: 1 [38400/60000 (64%)]\tLoss: 18.190182\n",
      "Train Epoch: 1 [39040/60000 (65%)]\tLoss: 25.395237\n",
      "Train Epoch: 1 [39680/60000 (66%)]\tLoss: 15.266775\n",
      "Train Epoch: 1 [40320/60000 (67%)]\tLoss: 15.307547\n",
      "Train Epoch: 1 [40960/60000 (68%)]\tLoss: 7.157564\n",
      "Train Epoch: 1 [41600/60000 (69%)]\tLoss: 18.285168\n",
      "Train Epoch: 1 [42240/60000 (70%)]\tLoss: 7.926974\n",
      "Train Epoch: 1 [42880/60000 (71%)]\tLoss: 6.831196\n",
      "Train Epoch: 1 [43520/60000 (72%)]\tLoss: 15.634546\n",
      "Train Epoch: 1 [44160/60000 (74%)]\tLoss: 25.840158\n",
      "Train Epoch: 1 [44800/60000 (75%)]\tLoss: 17.709942\n",
      "Train Epoch: 1 [45440/60000 (76%)]\tLoss: 29.772718\n",
      "Train Epoch: 1 [46080/60000 (77%)]\tLoss: 2.484915\n",
      "Train Epoch: 1 [46720/60000 (78%)]\tLoss: 22.630938\n",
      "Train Epoch: 1 [47360/60000 (79%)]\tLoss: 6.630656\n",
      "Train Epoch: 1 [48000/60000 (80%)]\tLoss: 6.235028\n",
      "Train Epoch: 1 [48640/60000 (81%)]\tLoss: 8.298281\n",
      "Train Epoch: 1 [49280/60000 (82%)]\tLoss: 24.254805\n",
      "Train Epoch: 1 [49920/60000 (83%)]\tLoss: 27.750019\n",
      "Train Epoch: 1 [50560/60000 (84%)]\tLoss: 29.090755\n",
      "Train Epoch: 1 [51200/60000 (85%)]\tLoss: 19.902773\n",
      "Train Epoch: 1 [51840/60000 (86%)]\tLoss: 8.825241\n",
      "Train Epoch: 1 [52480/60000 (87%)]\tLoss: 3.877952\n",
      "Train Epoch: 1 [53120/60000 (88%)]\tLoss: 13.929617\n",
      "Train Epoch: 1 [53760/60000 (90%)]\tLoss: 12.304418\n",
      "Train Epoch: 1 [54400/60000 (91%)]\tLoss: 15.165339\n",
      "Train Epoch: 1 [55040/60000 (92%)]\tLoss: 10.849617\n",
      "Train Epoch: 1 [55680/60000 (93%)]\tLoss: 11.191125\n",
      "Train Epoch: 1 [56320/60000 (94%)]\tLoss: 24.977251\n",
      "Train Epoch: 1 [56960/60000 (95%)]\tLoss: 16.535215\n",
      "Train Epoch: 1 [57600/60000 (96%)]\tLoss: 9.447549\n",
      "Train Epoch: 1 [58240/60000 (97%)]\tLoss: 9.000114\n",
      "Train Epoch: 1 [58880/60000 (98%)]\tLoss: 13.622834\n",
      "Train Epoch: 1 [59520/60000 (99%)]\tLoss: 25.350550\n",
      "\n",
      "Test set: Average loss: 0.2236, Accuracy: 9272/10000 (92.72%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 5. Perform training over multiple epochs\n",
    "\n",
    "# start training\n",
    "for epoch in range(1, epochs + 1):\n",
    "    train(epoch)\n",
    "    test(epoch)\n",
    "\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-e7b0279fa07794db\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-e7b0279fa07794db\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          const port = 8008;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%reload_ext tensorboard\n",
    "%tensorboard --logdir log_dir --port=8008"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
