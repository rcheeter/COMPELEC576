{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **mnist_convnet**\n",
    "- ELEC 576 HW 1\n",
    "- Robert Heeter\n",
    "- 4 October 2023\n",
    "\n",
    "## **Structure**:\n",
    "1) Set PyTorch metadata\n",
    "    - Seed\n",
    "    - TensorFlow output\n",
    "    - Whether to transfer to gpu (cuda)\n",
    "2) Import data\n",
    "    - Download data\n",
    "    - Create data loaders with batchsize, transforms, scaling\n",
    "3) Define model architecture, loss, and optimizer\n",
    "4) Define test and training loops\n",
    "    - Train:\n",
    "        - Get next batch\n",
    "        - Forward pass through model-\n",
    "        - Calculate loss\n",
    "        - Backward pass from loss (calculates the gradient for each parameter)\n",
    "        - Optimizer: performs weight updates\n",
    "5) Perform training over multiple epochs\n",
    "    - Each epoch:\n",
    "        - Call train loop\n",
    "        - Call test loop\n",
    "\n",
    "## **Acknowledgements**:\n",
    "- https://colab.research.google.com/drive/1i9KpbQyFU4zfq8zLLns8a2Kd8PRMGsaZ\n",
    "- https://github.com/motokimura/pytorch_tensorboard/blob/master/main.py\n",
    "- https://colab.research.google.com/github/tensorflow/tensorboard/blob/master/docs/tensorboard_in_notebooks.ipynb#scrollTo=lpUO9HqUKP6z\n",
    "- https://github.com/christianversloot/machine-learning-articles/blob/main/how-to-use-tensorboard-with-pytorch.md\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-06 01:46:54.245715: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.autograd import Variable\n",
    "import numpy as np\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from datetime import datetime\n",
    "import os\n",
    "\n",
    "%load_ext tensorboard\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Set PyTorch metadata\n",
    "\n",
    "batch_size = 64\n",
    "test_batch_size = 1000\n",
    "epochs = 10\n",
    "lr = 0.01\n",
    "try_cuda = True\n",
    "seed = 1000\n",
    "logging_interval = 10 # how many batches to wait before logging\n",
    "logging_dir = None\n",
    "\n",
    "# setting up the logging\n",
    "log_dir = os.path.join(os.getcwd(),'log/MNIST', datetime.now().strftime('%b%d_%H-%M-%S'))\n",
    "writer = SummaryWriter(log_dir=log_dir)\n",
    "\n",
    "# deciding whether to send to the cpu or not if available\n",
    "if torch.cuda.is_available() and try_cuda:\n",
    "    cuda = True\n",
    "    torch.cuda.mnaual_seed(seed)\n",
    "else:\n",
    "    cuda = False\n",
    "    torch.manual_seed(seed)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Import data\n",
    "\n",
    "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.01307,), (0.3081,))])\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(datasets.MNIST('data', train=True, download=True, transform=transform),\n",
    "                                           batch_size=batch_size,\n",
    "                                           shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(datasets.MNIST('data', train=False, download=True, transform=transform),\n",
    "                                          batch_size=test_batch_size,\n",
    "                                          shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (layers): Sequential(\n",
      "    (0): Conv2d(1, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "    (1): Tanh()\n",
      "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (3): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "    (4): Tanh()\n",
      "    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (6): Flatten(start_dim=1, end_dim=-1)\n",
      "    (7): Linear(in_features=320, out_features=50, bias=True)\n",
      "    (8): Tanh()\n",
      "    (9): Dropout2d(p=0.5, inplace=False)\n",
      "    (10): Linear(in_features=50, out_features=10, bias=True)\n",
      "    (11): Softmax(dim=1)\n",
      "  )\n",
      ")\n",
      "SGD (\n",
      "Parameter Group 0\n",
      "    dampening: 0\n",
      "    differentiable: False\n",
      "    foreach: None\n",
      "    lr: 0.01\n",
      "    maximize: False\n",
      "    momentum: 0\n",
      "    nesterov: False\n",
      "    weight_decay: 0\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# 3. Defining model architecture, loss, and optimizer\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        \n",
    "        self.layers = nn.Sequential(nn.Conv2d(1, 10, kernel_size=5),\n",
    "                                    nn.Tanh(),\n",
    "                                    nn.MaxPool2d(2),\n",
    "                                    nn.Conv2d(10, 20, kernel_size=5),\n",
    "                                    nn.Tanh(),\n",
    "                                    nn.MaxPool2d(2),\n",
    "                                    nn.Flatten(),\n",
    "                                    nn.Linear(320, 50),\n",
    "                                    nn.Tanh(),\n",
    "                                    nn.Dropout2d(0.5),\n",
    "                                    nn.Linear(50, 10),\n",
    "                                    nn.Softmax(dim=1)\n",
    "                                   )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        '''Forward pass'''\n",
    "        return self.layers(x)\n",
    "\n",
    "\n",
    "#         self.conv1 = nn.Conv2d(1, 10, kernel_size=5)\n",
    "#         self.conv2 = nn.Conv2d(10, 20, kernel_size=5)\n",
    "#         self.conv2_drop = nn.Dropout2d()\n",
    "#         self.fc1 = nn.Linear(320, 50)\n",
    "#         self.fc2 = nn.Linear(50, 10)\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         # original network architecture\n",
    "#         # x = F.relu(F.max_pool2d(self.conv1(x), 2))\n",
    "#         # x = F.relu(F.max_pool2d(self.conv2_drop(self.conv2(x)), 2))\n",
    "#         # x = x.view(-1, 320) # (batch_size, units)\n",
    "#         # x = F.relu(self.fc1(x))\n",
    "#         # x = F.dropout(x, training=self.training)\n",
    "#         # x = self.fc2(x)\n",
    "#         # x = F.softmax(x, dim=1)\n",
    "\n",
    "#         # new network architecture\n",
    "#         x = self.conv1(x)\n",
    "#         x = F.relu(x)\n",
    "#         x = F.max_pool2d(x, 2)\n",
    "#         x = self.conv2(x)\n",
    "#         x = F.relu(x)\n",
    "#         x = F.max_pool2d(x, 2)\n",
    "#         x = x.view(-1, 320)\n",
    "#         x = self.fc1(x)\n",
    "#         x = F.relu(x)\n",
    "#         x = F.dropout(x, p=0.5)\n",
    "#         x = self.fc2(x)\n",
    "#         x = F.softmax(x, dim=1)\n",
    "\n",
    "#         return x\n",
    "\n",
    "model = Net()\n",
    "print(model)\n",
    "\n",
    "optimizer = optim.SGD(model.parameters(), lr=lr)\n",
    "print(optimizer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weight_histograms_conv2d(writer, n_iter, weights, biases, layer_number):\n",
    "    weights_shape = weights.shape\n",
    "    num_kernels = weights_shape[0]\n",
    "    \n",
    "    for k in range(num_kernels):\n",
    "        \n",
    "        flattened_weights = weights[k].flatten()\n",
    "        tag = f\"layer_{layer_number}/kernel_{k}_weight_histogram\"\n",
    "        writer.add_histogram(tag, flattened_weights, n_iter, bins='tensorflow')\n",
    "        \n",
    "        tag = f\"layer_{layer_number}/kernel_{k}_weight_min\"\n",
    "        writer.add_scalar(tag, torch.min(flattened_weights), n_iter)\n",
    "        tag = f\"layer_{layer_number}/kernel_{k}_weight_max\"\n",
    "        writer.add_scalar(tag, torch.max(flattened_weights), n_iter)\n",
    "        tag = f\"layer_{layer_number}/kernel_{k}_weight_mean\"\n",
    "        writer.add_scalar(tag, torch.mean(flattened_weights), n_iter)\n",
    "        tag = f\"layer_{layer_number}/kernel_{k}_weight_stdev\"\n",
    "        writer.add_scalar(tag, torch.std(flattened_weights), n_iter)\n",
    "        \n",
    "        flattened_biases = biases[k].flatten()\n",
    "        tag = f\"layer_{layer_number}/kernel_{k}_biases_histogram\"\n",
    "        writer.add_histogram(tag, flattened_biases, n_iter, bins='tensorflow')\n",
    "        \n",
    "        tag = f\"layer_{layer_number}/kernel_{k}_biases_min\"\n",
    "        writer.add_scalar(tag, torch.min(flattened_biases), n_iter)\n",
    "        tag = f\"layer_{layer_number}/kernel_{k}_biases_max\"\n",
    "        writer.add_scalar(tag, torch.max(flattened_biases), n_iter)\n",
    "        tag = f\"layer_{layer_number}/kernel_{k}_biases_mean\"\n",
    "        writer.add_scalar(tag, torch.mean(flattened_biases), n_iter)\n",
    "        tag = f\"layer_{layer_number}/kernel_{k}_biases_stdev\"\n",
    "        writer.add_scalar(tag, torch.std(flattened_biases), n_iter)\n",
    "        \n",
    "        break\n",
    "        \n",
    "def weight_histograms_linear(writer, n_iter, weights, biases, layer_number):\n",
    "    flattened_weights = weights.flatten()\n",
    "    tag = f\"layer_{layer_number}_weight_histogram\"\n",
    "    writer.add_histogram(tag, flattened_weights, n_iter, bins='tensorflow')\n",
    "    \n",
    "    tag = f\"layer_{layer_number}_weight_min\"\n",
    "    writer.add_scalar(tag, torch.min(flattened_weights), n_iter)\n",
    "    tag = f\"layer_{layer_number}_weight_max\"\n",
    "    writer.add_scalar(tag, torch.max(flattened_weights), n_iter)\n",
    "    tag = f\"layer_{layer_number}_weight_mean\"\n",
    "    writer.add_scalar(tag, torch.mean(flattened_weights), n_iter)\n",
    "    tag = f\"layer_{layer_number}_weight_stdev\"\n",
    "    writer.add_scalar(tag, torch.std(flattened_weights), n_iter)\n",
    "\n",
    "    flattened_biases = biases.flatten()\n",
    "    tag = f\"layer_{layer_number}_biases_histogram\"\n",
    "    writer.add_histogram(tag, flattened_biases, n_iter, bins='tensorflow')\n",
    "\n",
    "    tag = f\"layer_{layer_number}_biases_min\"\n",
    "    writer.add_scalar(tag, torch.min(flattened_biases), n_iter)\n",
    "    tag = f\"layer_{layer_number}_biases_max\"\n",
    "    writer.add_scalar(tag, torch.max(flattened_biases), n_iter)\n",
    "    tag = f\"layer_{layer_number}_biases_mean\"\n",
    "    writer.add_scalar(tag, torch.mean(flattened_biases), n_iter)\n",
    "    tag = f\"layer_{layer_number}_biases_stdev\"\n",
    "    writer.add_scalar(tag, torch.std(flattened_biases), n_iter)\n",
    "\n",
    "def weight_histograms(writer, n_iter, model):\n",
    "    # iterate over all model layers\n",
    "    for layer_number in range(len(model.layers)):\n",
    "        \n",
    "        # get layer\n",
    "        layer = model.layers[layer_number]\n",
    "        \n",
    "        # compute weight histograms for appropriate layer\n",
    "        if isinstance(layer, nn.Conv2d):\n",
    "            weights = layer.weight\n",
    "            biases = layer.bias\n",
    "            weight_histograms_conv2d(writer, n_iter, weights, biases, layer_number)\n",
    "            \n",
    "        elif isinstance(layer, nn.Linear):\n",
    "            weights = layer.weight\n",
    "            biases = layer.bias\n",
    "            weight_histograms_linear(writer, n_iter, weights, biases, layer_number)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Define test and training loops\n",
    "\n",
    "eps = 1e-13\n",
    "\n",
    "def train(epoch):\n",
    "    model.train()\n",
    "\n",
    "    # criterion = nn.CrossEntropyLoss()\n",
    "    criterion = nn.NLLLoss(size_average=False)\n",
    "    \n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        if cuda:\n",
    "            data, target = data.cuda(), target.cuda()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data) # forward pass\n",
    "        loss = criterion(torch.log(output+eps), target) # = sum_k(-t_k * log(y_k))\n",
    "        loss.backward() # backward pass\n",
    "        optimizer.step()\n",
    "\n",
    "        if batch_idx % logging_interval == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), loss.data)\n",
    "            )\n",
    "\n",
    "            # log train/loss to TensorBoard at every iteration\n",
    "            n_iter = (epoch - 1) * len(train_loader) + batch_idx + 1\n",
    "            writer.add_scalar('train/loss', loss.data, n_iter)\n",
    "                    \n",
    "    # visualize layer weights and biases\n",
    "    weight_histograms(writer, n_iter, model)\n",
    "    \n",
    "    # log model parameters to TensorBoard at every epoch\n",
    "    for name, param in model.named_parameters():\n",
    "        layer, attr = os.path.splitext(name)\n",
    "        attr = attr[1:]\n",
    "        writer.add_histogram('{}/{}'.format(layer, attr), param.clone().cpu().data.numpy(), n_iter)\n",
    "\n",
    "def test(epoch):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    \n",
    "    # criterion = nn.CrossEntropyLoss()\n",
    "    # criterion = nn.CrossEntropyLoss(size_average = False)\n",
    "    criterion = nn.NLLLoss(size_average = False)\n",
    "\n",
    "    for data, target in test_loader:\n",
    "        if cuda:\n",
    "            data, target = data.cuda(), target.cuda()\n",
    "\n",
    "        output = model(data)\n",
    "\n",
    "        test_loss += criterion(torch.log(output+eps), target,).item() # sum up batch loss (later, averaged over all test samples)\n",
    "        pred = output.data.max(1, keepdim=True)[1] # get the index of the max log-probability\n",
    "        correct += pred.eq(target.data.view_as(pred)).cpu().sum()\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    test_accuracy = 100. * correct / len(test_loader.dataset)\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.2f}%)\\n'.format(\n",
    "        test_loss, correct, len(test_loader.dataset), test_accuracy)\n",
    "    )\n",
    "\n",
    "    # log test/loss and test/accuracy to TensorBoard at every epoch\n",
    "    n_iter = epoch * len(train_loader)\n",
    "    writer.add_scalar('test/loss', test_loss, n_iter)\n",
    "    writer.add_scalar('test/accuracy', test_accuracy, n_iter)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rch/opt/anaconda3/lib/python3.9/site-packages/torch/nn/_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.\n",
      "  warnings.warn(warning.format(ret))\n",
      "/Users/rch/opt/anaconda3/lib/python3.9/site-packages/torch/nn/functional.py:1345: UserWarning: dropout2d: Received a 2-D input to dropout2d, which is deprecated and will result in an error in a future release. To retain the behavior and silence this warning, please use dropout instead. Note that dropout2d exists to provide channel-wise dropout on inputs with 2 spatial dimensions, a channel dimension, and an optional batch dimension (i.e. 3D or 4D inputs).\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: 148.350174\n",
      "Train Epoch: 1 [640/60000 (1%)]\tLoss: 65.574791\n",
      "Train Epoch: 1 [1280/60000 (2%)]\tLoss: 50.805614\n",
      "Train Epoch: 1 [1920/60000 (3%)]\tLoss: 27.808430\n",
      "Train Epoch: 1 [2560/60000 (4%)]\tLoss: 29.381617\n",
      "Train Epoch: 1 [3200/60000 (5%)]\tLoss: 47.149063\n",
      "Train Epoch: 1 [3840/60000 (6%)]\tLoss: 27.568836\n",
      "Train Epoch: 1 [4480/60000 (7%)]\tLoss: 25.695684\n",
      "Train Epoch: 1 [5120/60000 (9%)]\tLoss: 32.483009\n",
      "Train Epoch: 1 [5760/60000 (10%)]\tLoss: 28.119480\n",
      "Train Epoch: 1 [6400/60000 (11%)]\tLoss: 16.325533\n",
      "Train Epoch: 1 [7040/60000 (12%)]\tLoss: 21.478676\n",
      "Train Epoch: 1 [7680/60000 (13%)]\tLoss: 23.415968\n",
      "Train Epoch: 1 [8320/60000 (14%)]\tLoss: 21.872169\n",
      "Train Epoch: 1 [8960/60000 (15%)]\tLoss: 13.121871\n",
      "Train Epoch: 1 [9600/60000 (16%)]\tLoss: 13.346699\n",
      "Train Epoch: 1 [10240/60000 (17%)]\tLoss: 30.787872\n",
      "Train Epoch: 1 [10880/60000 (18%)]\tLoss: 22.634539\n",
      "Train Epoch: 1 [11520/60000 (19%)]\tLoss: 25.395401\n",
      "Train Epoch: 1 [12160/60000 (20%)]\tLoss: 14.447589\n",
      "Train Epoch: 1 [12800/60000 (21%)]\tLoss: 10.730078\n",
      "Train Epoch: 1 [13440/60000 (22%)]\tLoss: 12.101591\n",
      "Train Epoch: 1 [14080/60000 (23%)]\tLoss: 55.785133\n",
      "Train Epoch: 1 [14720/60000 (25%)]\tLoss: 10.913371\n",
      "Train Epoch: 1 [15360/60000 (26%)]\tLoss: 15.996431\n",
      "Train Epoch: 1 [16000/60000 (27%)]\tLoss: 12.564799\n",
      "Train Epoch: 1 [16640/60000 (28%)]\tLoss: 10.983263\n",
      "Train Epoch: 1 [17280/60000 (29%)]\tLoss: 8.856709\n",
      "Train Epoch: 1 [17920/60000 (30%)]\tLoss: 8.925025\n",
      "Train Epoch: 1 [18560/60000 (31%)]\tLoss: 13.580734\n",
      "Train Epoch: 1 [19200/60000 (32%)]\tLoss: 23.908947\n",
      "Train Epoch: 1 [19840/60000 (33%)]\tLoss: 9.361331\n",
      "Train Epoch: 1 [20480/60000 (34%)]\tLoss: 26.058790\n",
      "Train Epoch: 1 [21120/60000 (35%)]\tLoss: 20.889919\n",
      "Train Epoch: 1 [21760/60000 (36%)]\tLoss: 16.410866\n",
      "Train Epoch: 1 [22400/60000 (37%)]\tLoss: 20.695486\n",
      "Train Epoch: 1 [23040/60000 (38%)]\tLoss: 8.645380\n",
      "Train Epoch: 1 [23680/60000 (39%)]\tLoss: 2.316973\n",
      "Train Epoch: 1 [24320/60000 (41%)]\tLoss: 12.669093\n",
      "Train Epoch: 1 [24960/60000 (42%)]\tLoss: 12.652076\n",
      "Train Epoch: 1 [25600/60000 (43%)]\tLoss: 6.577795\n",
      "Train Epoch: 1 [26240/60000 (44%)]\tLoss: 15.498992\n",
      "Train Epoch: 1 [26880/60000 (45%)]\tLoss: 10.603088\n",
      "Train Epoch: 1 [27520/60000 (46%)]\tLoss: 6.678514\n",
      "Train Epoch: 1 [28160/60000 (47%)]\tLoss: 4.343204\n",
      "Train Epoch: 1 [28800/60000 (48%)]\tLoss: 26.278395\n",
      "Train Epoch: 1 [29440/60000 (49%)]\tLoss: 9.498826\n",
      "Train Epoch: 1 [30080/60000 (50%)]\tLoss: 11.930044\n",
      "Train Epoch: 1 [30720/60000 (51%)]\tLoss: 16.838755\n",
      "Train Epoch: 1 [31360/60000 (52%)]\tLoss: 7.070224\n",
      "Train Epoch: 1 [32000/60000 (53%)]\tLoss: 18.772337\n",
      "Train Epoch: 1 [32640/60000 (54%)]\tLoss: 17.717880\n",
      "Train Epoch: 1 [33280/60000 (55%)]\tLoss: 6.297252\n",
      "Train Epoch: 1 [33920/60000 (57%)]\tLoss: 15.736999\n",
      "Train Epoch: 1 [34560/60000 (58%)]\tLoss: 9.797343\n",
      "Train Epoch: 1 [35200/60000 (59%)]\tLoss: 6.874809\n",
      "Train Epoch: 1 [35840/60000 (60%)]\tLoss: 16.641665\n",
      "Train Epoch: 1 [36480/60000 (61%)]\tLoss: 18.873581\n",
      "Train Epoch: 1 [37120/60000 (62%)]\tLoss: 10.327193\n",
      "Train Epoch: 1 [37760/60000 (63%)]\tLoss: 10.541352\n",
      "Train Epoch: 1 [38400/60000 (64%)]\tLoss: 20.968702\n",
      "Train Epoch: 1 [39040/60000 (65%)]\tLoss: 14.714314\n",
      "Train Epoch: 1 [39680/60000 (66%)]\tLoss: 8.235843\n",
      "Train Epoch: 1 [40320/60000 (67%)]\tLoss: 8.279308\n",
      "Train Epoch: 1 [40960/60000 (68%)]\tLoss: 12.288799\n",
      "Train Epoch: 1 [41600/60000 (69%)]\tLoss: 2.113179\n",
      "Train Epoch: 1 [42240/60000 (70%)]\tLoss: 12.241662\n",
      "Train Epoch: 1 [42880/60000 (71%)]\tLoss: 9.805692\n",
      "Train Epoch: 1 [43520/60000 (72%)]\tLoss: 18.253258\n",
      "Train Epoch: 1 [44160/60000 (74%)]\tLoss: 10.824991\n",
      "Train Epoch: 1 [44800/60000 (75%)]\tLoss: 5.566197\n",
      "Train Epoch: 1 [45440/60000 (76%)]\tLoss: 14.457938\n",
      "Train Epoch: 1 [46080/60000 (77%)]\tLoss: 10.472195\n",
      "Train Epoch: 1 [46720/60000 (78%)]\tLoss: 1.622838\n",
      "Train Epoch: 1 [47360/60000 (79%)]\tLoss: 32.133198\n",
      "Train Epoch: 1 [48000/60000 (80%)]\tLoss: 22.061737\n",
      "Train Epoch: 1 [48640/60000 (81%)]\tLoss: 6.789975\n",
      "Train Epoch: 1 [49280/60000 (82%)]\tLoss: 23.259237\n",
      "Train Epoch: 1 [49920/60000 (83%)]\tLoss: 9.818250\n",
      "Train Epoch: 1 [50560/60000 (84%)]\tLoss: 8.120830\n",
      "Train Epoch: 1 [51200/60000 (85%)]\tLoss: 19.550013\n",
      "Train Epoch: 1 [51840/60000 (86%)]\tLoss: 7.249803\n",
      "Train Epoch: 1 [52480/60000 (87%)]\tLoss: 11.707809\n",
      "Train Epoch: 1 [53120/60000 (88%)]\tLoss: 10.630896\n",
      "Train Epoch: 1 [53760/60000 (90%)]\tLoss: 7.630034\n",
      "Train Epoch: 1 [54400/60000 (91%)]\tLoss: 9.601753\n",
      "Train Epoch: 1 [55040/60000 (92%)]\tLoss: 5.875074\n",
      "Train Epoch: 1 [55680/60000 (93%)]\tLoss: 16.653101\n",
      "Train Epoch: 1 [56320/60000 (94%)]\tLoss: 7.744590\n",
      "Train Epoch: 1 [56960/60000 (95%)]\tLoss: 10.115010\n",
      "Train Epoch: 1 [57600/60000 (96%)]\tLoss: 8.761250\n",
      "Train Epoch: 1 [58240/60000 (97%)]\tLoss: 2.970207\n",
      "Train Epoch: 1 [58880/60000 (98%)]\tLoss: 4.338736\n",
      "Train Epoch: 1 [59520/60000 (99%)]\tLoss: 10.248787\n",
      "\n",
      "Test set: Average loss: 0.0816, Accuracy: 9742/10000 (97.42%)\n",
      "\n",
      "Train Epoch: 2 [0/60000 (0%)]\tLoss: 5.794899\n",
      "Train Epoch: 2 [640/60000 (1%)]\tLoss: 7.820867\n",
      "Train Epoch: 2 [1280/60000 (2%)]\tLoss: 9.429663\n",
      "Train Epoch: 2 [1920/60000 (3%)]\tLoss: 3.547286\n",
      "Train Epoch: 2 [2560/60000 (4%)]\tLoss: 8.738937\n",
      "Train Epoch: 2 [3200/60000 (5%)]\tLoss: 12.709581\n",
      "Train Epoch: 2 [3840/60000 (6%)]\tLoss: 6.283051\n",
      "Train Epoch: 2 [4480/60000 (7%)]\tLoss: 10.825681\n",
      "Train Epoch: 2 [5120/60000 (9%)]\tLoss: 1.381034\n",
      "Train Epoch: 2 [5760/60000 (10%)]\tLoss: 9.333070\n",
      "Train Epoch: 2 [6400/60000 (11%)]\tLoss: 6.135649\n",
      "Train Epoch: 2 [7040/60000 (12%)]\tLoss: 7.319004\n",
      "Train Epoch: 2 [7680/60000 (13%)]\tLoss: 3.200693\n",
      "Train Epoch: 2 [8320/60000 (14%)]\tLoss: 3.356311\n",
      "Train Epoch: 2 [8960/60000 (15%)]\tLoss: 18.460999\n",
      "Train Epoch: 2 [9600/60000 (16%)]\tLoss: 12.877852\n",
      "Train Epoch: 2 [10240/60000 (17%)]\tLoss: 4.580849\n",
      "Train Epoch: 2 [10880/60000 (18%)]\tLoss: 7.309263\n",
      "Train Epoch: 2 [11520/60000 (19%)]\tLoss: 9.147858\n",
      "Train Epoch: 2 [12160/60000 (20%)]\tLoss: 8.414282\n",
      "Train Epoch: 2 [12800/60000 (21%)]\tLoss: 2.468552\n",
      "Train Epoch: 2 [13440/60000 (22%)]\tLoss: 11.544351\n",
      "Train Epoch: 2 [14080/60000 (23%)]\tLoss: 5.105357\n",
      "Train Epoch: 2 [14720/60000 (25%)]\tLoss: 2.410307\n",
      "Train Epoch: 2 [15360/60000 (26%)]\tLoss: 1.240903\n",
      "Train Epoch: 2 [16000/60000 (27%)]\tLoss: 3.651239\n",
      "Train Epoch: 2 [16640/60000 (28%)]\tLoss: 14.097479\n",
      "Train Epoch: 2 [17280/60000 (29%)]\tLoss: 11.660074\n",
      "Train Epoch: 2 [17920/60000 (30%)]\tLoss: 9.857907\n",
      "Train Epoch: 2 [18560/60000 (31%)]\tLoss: 11.347275\n",
      "Train Epoch: 2 [19200/60000 (32%)]\tLoss: 15.710032\n",
      "Train Epoch: 2 [19840/60000 (33%)]\tLoss: 7.412633\n",
      "Train Epoch: 2 [20480/60000 (34%)]\tLoss: 9.905286\n",
      "Train Epoch: 2 [21120/60000 (35%)]\tLoss: 5.549774\n",
      "Train Epoch: 2 [21760/60000 (36%)]\tLoss: 16.673695\n",
      "Train Epoch: 2 [22400/60000 (37%)]\tLoss: 15.923005\n",
      "Train Epoch: 2 [23040/60000 (38%)]\tLoss: 10.912141\n",
      "Train Epoch: 2 [23680/60000 (39%)]\tLoss: 9.906033\n",
      "Train Epoch: 2 [24320/60000 (41%)]\tLoss: 4.116310\n",
      "Train Epoch: 2 [24960/60000 (42%)]\tLoss: 1.313582\n",
      "Train Epoch: 2 [25600/60000 (43%)]\tLoss: 6.027876\n",
      "Train Epoch: 2 [26240/60000 (44%)]\tLoss: 11.249135\n",
      "Train Epoch: 2 [26880/60000 (45%)]\tLoss: 10.030438\n",
      "Train Epoch: 2 [27520/60000 (46%)]\tLoss: 8.641503\n",
      "Train Epoch: 2 [28160/60000 (47%)]\tLoss: 5.242270\n",
      "Train Epoch: 2 [28800/60000 (48%)]\tLoss: 3.222556\n",
      "Train Epoch: 2 [29440/60000 (49%)]\tLoss: 4.121618\n",
      "Train Epoch: 2 [30080/60000 (50%)]\tLoss: 5.472337\n",
      "Train Epoch: 2 [30720/60000 (51%)]\tLoss: 9.213414\n",
      "Train Epoch: 2 [31360/60000 (52%)]\tLoss: 3.990453\n",
      "Train Epoch: 2 [32000/60000 (53%)]\tLoss: 4.348527\n",
      "Train Epoch: 2 [32640/60000 (54%)]\tLoss: 4.675620\n",
      "Train Epoch: 2 [33280/60000 (55%)]\tLoss: 21.696838\n",
      "Train Epoch: 2 [33920/60000 (57%)]\tLoss: 0.893426\n",
      "Train Epoch: 2 [34560/60000 (58%)]\tLoss: 5.837167\n",
      "Train Epoch: 2 [35200/60000 (59%)]\tLoss: 5.856362\n",
      "Train Epoch: 2 [35840/60000 (60%)]\tLoss: 6.947505\n",
      "Train Epoch: 2 [36480/60000 (61%)]\tLoss: 9.290040\n",
      "Train Epoch: 2 [37120/60000 (62%)]\tLoss: 11.697110\n",
      "Train Epoch: 2 [37760/60000 (63%)]\tLoss: 12.097234\n",
      "Train Epoch: 2 [38400/60000 (64%)]\tLoss: 6.305795\n",
      "Train Epoch: 2 [39040/60000 (65%)]\tLoss: 2.917878\n",
      "Train Epoch: 2 [39680/60000 (66%)]\tLoss: 7.600556\n",
      "Train Epoch: 2 [40320/60000 (67%)]\tLoss: 4.543109\n",
      "Train Epoch: 2 [40960/60000 (68%)]\tLoss: 6.848329\n",
      "Train Epoch: 2 [41600/60000 (69%)]\tLoss: 11.867298\n",
      "Train Epoch: 2 [42240/60000 (70%)]\tLoss: 6.556248\n",
      "Train Epoch: 2 [42880/60000 (71%)]\tLoss: 10.470507\n",
      "Train Epoch: 2 [43520/60000 (72%)]\tLoss: 2.900258\n",
      "Train Epoch: 2 [44160/60000 (74%)]\tLoss: 10.441376\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 2 [44800/60000 (75%)]\tLoss: 7.591688\n",
      "Train Epoch: 2 [45440/60000 (76%)]\tLoss: 16.178593\n",
      "Train Epoch: 2 [46080/60000 (77%)]\tLoss: 6.151915\n",
      "Train Epoch: 2 [46720/60000 (78%)]\tLoss: 16.359280\n",
      "Train Epoch: 2 [47360/60000 (79%)]\tLoss: 16.866432\n",
      "Train Epoch: 2 [48000/60000 (80%)]\tLoss: 5.174510\n",
      "Train Epoch: 2 [48640/60000 (81%)]\tLoss: 7.196651\n",
      "Train Epoch: 2 [49280/60000 (82%)]\tLoss: 6.357285\n",
      "Train Epoch: 2 [49920/60000 (83%)]\tLoss: 16.031305\n",
      "Train Epoch: 2 [50560/60000 (84%)]\tLoss: 5.842550\n",
      "Train Epoch: 2 [51200/60000 (85%)]\tLoss: 20.573612\n",
      "Train Epoch: 2 [51840/60000 (86%)]\tLoss: 5.943273\n",
      "Train Epoch: 2 [52480/60000 (87%)]\tLoss: 2.580393\n",
      "Train Epoch: 2 [53120/60000 (88%)]\tLoss: 7.100457\n",
      "Train Epoch: 2 [53760/60000 (90%)]\tLoss: 26.377951\n",
      "Train Epoch: 2 [54400/60000 (91%)]\tLoss: 7.567432\n",
      "Train Epoch: 2 [55040/60000 (92%)]\tLoss: 3.035346\n",
      "Train Epoch: 2 [55680/60000 (93%)]\tLoss: 2.375730\n",
      "Train Epoch: 2 [56320/60000 (94%)]\tLoss: 12.264088\n",
      "Train Epoch: 2 [56960/60000 (95%)]\tLoss: 8.824215\n",
      "Train Epoch: 2 [57600/60000 (96%)]\tLoss: 5.415017\n",
      "Train Epoch: 2 [58240/60000 (97%)]\tLoss: 3.947428\n",
      "Train Epoch: 2 [58880/60000 (98%)]\tLoss: 5.410202\n",
      "Train Epoch: 2 [59520/60000 (99%)]\tLoss: 4.062138\n",
      "\n",
      "Test set: Average loss: 0.0692, Accuracy: 9783/10000 (97.83%)\n",
      "\n",
      "Train Epoch: 3 [0/60000 (0%)]\tLoss: 2.392176\n",
      "Train Epoch: 3 [640/60000 (1%)]\tLoss: 23.679876\n",
      "Train Epoch: 3 [1280/60000 (2%)]\tLoss: 5.030544\n",
      "Train Epoch: 3 [1920/60000 (3%)]\tLoss: 19.759233\n",
      "Train Epoch: 3 [2560/60000 (4%)]\tLoss: 8.832271\n",
      "Train Epoch: 3 [3200/60000 (5%)]\tLoss: 13.441246\n",
      "Train Epoch: 3 [3840/60000 (6%)]\tLoss: 13.123025\n",
      "Train Epoch: 3 [4480/60000 (7%)]\tLoss: 7.412638\n",
      "Train Epoch: 3 [5120/60000 (9%)]\tLoss: 4.251581\n",
      "Train Epoch: 3 [5760/60000 (10%)]\tLoss: 5.962806\n",
      "Train Epoch: 3 [6400/60000 (11%)]\tLoss: 6.662619\n",
      "Train Epoch: 3 [7040/60000 (12%)]\tLoss: 2.393607\n",
      "Train Epoch: 3 [7680/60000 (13%)]\tLoss: 2.496614\n",
      "Train Epoch: 3 [8320/60000 (14%)]\tLoss: 1.160033\n",
      "Train Epoch: 3 [8960/60000 (15%)]\tLoss: 7.224338\n",
      "Train Epoch: 3 [9600/60000 (16%)]\tLoss: 6.754421\n",
      "Train Epoch: 3 [10240/60000 (17%)]\tLoss: 2.993656\n",
      "Train Epoch: 3 [10880/60000 (18%)]\tLoss: 9.229931\n",
      "Train Epoch: 3 [11520/60000 (19%)]\tLoss: 2.856031\n",
      "Train Epoch: 3 [12160/60000 (20%)]\tLoss: 12.549516\n",
      "Train Epoch: 3 [12800/60000 (21%)]\tLoss: 8.845797\n",
      "Train Epoch: 3 [13440/60000 (22%)]\tLoss: 1.208502\n",
      "Train Epoch: 3 [14080/60000 (23%)]\tLoss: 1.130532\n",
      "Train Epoch: 3 [14720/60000 (25%)]\tLoss: 3.245269\n",
      "Train Epoch: 3 [15360/60000 (26%)]\tLoss: 10.745577\n",
      "Train Epoch: 3 [16000/60000 (27%)]\tLoss: 13.248148\n",
      "Train Epoch: 3 [16640/60000 (28%)]\tLoss: 5.076397\n",
      "Train Epoch: 3 [17280/60000 (29%)]\tLoss: 6.256505\n",
      "Train Epoch: 3 [17920/60000 (30%)]\tLoss: 3.376264\n",
      "Train Epoch: 3 [18560/60000 (31%)]\tLoss: 4.911217\n",
      "Train Epoch: 3 [19200/60000 (32%)]\tLoss: 4.161876\n",
      "Train Epoch: 3 [19840/60000 (33%)]\tLoss: 5.511187\n",
      "Train Epoch: 3 [20480/60000 (34%)]\tLoss: 9.990060\n",
      "Train Epoch: 3 [21120/60000 (35%)]\tLoss: 6.755213\n",
      "Train Epoch: 3 [21760/60000 (36%)]\tLoss: 6.218166\n",
      "Train Epoch: 3 [22400/60000 (37%)]\tLoss: 9.202353\n",
      "Train Epoch: 3 [23040/60000 (38%)]\tLoss: 4.193927\n",
      "Train Epoch: 3 [23680/60000 (39%)]\tLoss: 3.067010\n",
      "Train Epoch: 3 [24320/60000 (41%)]\tLoss: 2.352905\n",
      "Train Epoch: 3 [24960/60000 (42%)]\tLoss: 0.969219\n",
      "Train Epoch: 3 [25600/60000 (43%)]\tLoss: 4.735764\n",
      "Train Epoch: 3 [26240/60000 (44%)]\tLoss: 8.341851\n",
      "Train Epoch: 3 [26880/60000 (45%)]\tLoss: 2.404269\n",
      "Train Epoch: 3 [27520/60000 (46%)]\tLoss: 0.940102\n",
      "Train Epoch: 3 [28160/60000 (47%)]\tLoss: 3.918036\n",
      "Train Epoch: 3 [28800/60000 (48%)]\tLoss: 12.873190\n",
      "Train Epoch: 3 [29440/60000 (49%)]\tLoss: 10.763255\n",
      "Train Epoch: 3 [30080/60000 (50%)]\tLoss: 4.989667\n",
      "Train Epoch: 3 [30720/60000 (51%)]\tLoss: 8.735826\n",
      "Train Epoch: 3 [31360/60000 (52%)]\tLoss: 1.024777\n",
      "Train Epoch: 3 [32000/60000 (53%)]\tLoss: 3.098973\n",
      "Train Epoch: 3 [32640/60000 (54%)]\tLoss: 12.592302\n",
      "Train Epoch: 3 [33280/60000 (55%)]\tLoss: 9.946776\n",
      "Train Epoch: 3 [33920/60000 (57%)]\tLoss: 4.094009\n",
      "Train Epoch: 3 [34560/60000 (58%)]\tLoss: 3.818836\n",
      "Train Epoch: 3 [35200/60000 (59%)]\tLoss: 1.338328\n",
      "Train Epoch: 3 [35840/60000 (60%)]\tLoss: 9.876770\n",
      "Train Epoch: 3 [36480/60000 (61%)]\tLoss: 3.634490\n",
      "Train Epoch: 3 [37120/60000 (62%)]\tLoss: 0.821941\n",
      "Train Epoch: 3 [37760/60000 (63%)]\tLoss: 5.335407\n",
      "Train Epoch: 3 [38400/60000 (64%)]\tLoss: 1.914721\n",
      "Train Epoch: 3 [39040/60000 (65%)]\tLoss: 9.632752\n",
      "Train Epoch: 3 [39680/60000 (66%)]\tLoss: 6.033620\n",
      "Train Epoch: 3 [40320/60000 (67%)]\tLoss: 6.842088\n",
      "Train Epoch: 3 [40960/60000 (68%)]\tLoss: 1.610606\n",
      "Train Epoch: 3 [41600/60000 (69%)]\tLoss: 1.251294\n",
      "Train Epoch: 3 [42240/60000 (70%)]\tLoss: 20.121061\n",
      "Train Epoch: 3 [42880/60000 (71%)]\tLoss: 7.533456\n",
      "Train Epoch: 3 [43520/60000 (72%)]\tLoss: 8.079223\n",
      "Train Epoch: 3 [44160/60000 (74%)]\tLoss: 2.490916\n",
      "Train Epoch: 3 [44800/60000 (75%)]\tLoss: 13.690059\n",
      "Train Epoch: 3 [45440/60000 (76%)]\tLoss: 17.511292\n",
      "Train Epoch: 3 [46080/60000 (77%)]\tLoss: 14.385363\n",
      "Train Epoch: 3 [46720/60000 (78%)]\tLoss: 6.217794\n",
      "Train Epoch: 3 [47360/60000 (79%)]\tLoss: 18.025816\n",
      "Train Epoch: 3 [48000/60000 (80%)]\tLoss: 4.146681\n",
      "Train Epoch: 3 [48640/60000 (81%)]\tLoss: 7.283103\n",
      "Train Epoch: 3 [49280/60000 (82%)]\tLoss: 8.716898\n",
      "Train Epoch: 3 [49920/60000 (83%)]\tLoss: 12.746178\n",
      "Train Epoch: 3 [50560/60000 (84%)]\tLoss: 10.398217\n",
      "Train Epoch: 3 [51200/60000 (85%)]\tLoss: 11.219976\n",
      "Train Epoch: 3 [51840/60000 (86%)]\tLoss: 8.326018\n",
      "Train Epoch: 3 [52480/60000 (87%)]\tLoss: 7.930647\n",
      "Train Epoch: 3 [53120/60000 (88%)]\tLoss: 4.383054\n",
      "Train Epoch: 3 [53760/60000 (90%)]\tLoss: 6.676928\n",
      "Train Epoch: 3 [54400/60000 (91%)]\tLoss: 5.456255\n",
      "Train Epoch: 3 [55040/60000 (92%)]\tLoss: 6.304915\n",
      "Train Epoch: 3 [55680/60000 (93%)]\tLoss: 6.002474\n",
      "Train Epoch: 3 [56320/60000 (94%)]\tLoss: 1.274454\n",
      "Train Epoch: 3 [56960/60000 (95%)]\tLoss: 0.718004\n",
      "Train Epoch: 3 [57600/60000 (96%)]\tLoss: 7.351045\n",
      "Train Epoch: 3 [58240/60000 (97%)]\tLoss: 1.383693\n",
      "Train Epoch: 3 [58880/60000 (98%)]\tLoss: 1.011876\n",
      "Train Epoch: 3 [59520/60000 (99%)]\tLoss: 19.323925\n",
      "\n",
      "Test set: Average loss: 0.0581, Accuracy: 9817/10000 (98.17%)\n",
      "\n",
      "Train Epoch: 4 [0/60000 (0%)]\tLoss: 1.760554\n",
      "Train Epoch: 4 [640/60000 (1%)]\tLoss: 10.024610\n",
      "Train Epoch: 4 [1280/60000 (2%)]\tLoss: 2.663712\n",
      "Train Epoch: 4 [1920/60000 (3%)]\tLoss: 6.489213\n",
      "Train Epoch: 4 [2560/60000 (4%)]\tLoss: 7.033193\n",
      "Train Epoch: 4 [3200/60000 (5%)]\tLoss: 2.532507\n",
      "Train Epoch: 4 [3840/60000 (6%)]\tLoss: 11.275025\n",
      "Train Epoch: 4 [4480/60000 (7%)]\tLoss: 11.003782\n",
      "Train Epoch: 4 [5120/60000 (9%)]\tLoss: 6.053628\n",
      "Train Epoch: 4 [5760/60000 (10%)]\tLoss: 3.514347\n",
      "Train Epoch: 4 [6400/60000 (11%)]\tLoss: 3.162746\n",
      "Train Epoch: 4 [7040/60000 (12%)]\tLoss: 1.572698\n",
      "Train Epoch: 4 [7680/60000 (13%)]\tLoss: 8.379597\n",
      "Train Epoch: 4 [8320/60000 (14%)]\tLoss: 1.317598\n",
      "Train Epoch: 4 [8960/60000 (15%)]\tLoss: 3.214137\n",
      "Train Epoch: 4 [9600/60000 (16%)]\tLoss: 12.529469\n",
      "Train Epoch: 4 [10240/60000 (17%)]\tLoss: 4.253539\n",
      "Train Epoch: 4 [10880/60000 (18%)]\tLoss: 5.843931\n",
      "Train Epoch: 4 [11520/60000 (19%)]\tLoss: 8.946067\n",
      "Train Epoch: 4 [12160/60000 (20%)]\tLoss: 8.974072\n",
      "Train Epoch: 4 [12800/60000 (21%)]\tLoss: 6.495608\n",
      "Train Epoch: 4 [13440/60000 (22%)]\tLoss: 2.123494\n",
      "Train Epoch: 4 [14080/60000 (23%)]\tLoss: 1.127161\n",
      "Train Epoch: 4 [14720/60000 (25%)]\tLoss: 4.231235\n",
      "Train Epoch: 4 [15360/60000 (26%)]\tLoss: 16.105898\n",
      "Train Epoch: 4 [16000/60000 (27%)]\tLoss: 4.430163\n",
      "Train Epoch: 4 [16640/60000 (28%)]\tLoss: 1.264707\n",
      "Train Epoch: 4 [17280/60000 (29%)]\tLoss: 1.233963\n",
      "Train Epoch: 4 [17920/60000 (30%)]\tLoss: 3.708989\n",
      "Train Epoch: 4 [18560/60000 (31%)]\tLoss: 15.077209\n",
      "Train Epoch: 4 [19200/60000 (32%)]\tLoss: 18.212618\n",
      "Train Epoch: 4 [19840/60000 (33%)]\tLoss: 2.751913\n",
      "Train Epoch: 4 [20480/60000 (34%)]\tLoss: 2.928551\n",
      "Train Epoch: 4 [21120/60000 (35%)]\tLoss: 7.999966\n",
      "Train Epoch: 4 [21760/60000 (36%)]\tLoss: 2.869562\n",
      "Train Epoch: 4 [22400/60000 (37%)]\tLoss: 9.410314\n",
      "Train Epoch: 4 [23040/60000 (38%)]\tLoss: 14.839697\n",
      "Train Epoch: 4 [23680/60000 (39%)]\tLoss: 5.865270\n",
      "Train Epoch: 4 [24320/60000 (41%)]\tLoss: 6.881284\n",
      "Train Epoch: 4 [24960/60000 (42%)]\tLoss: 12.897303\n",
      "Train Epoch: 4 [25600/60000 (43%)]\tLoss: 6.068412\n",
      "Train Epoch: 4 [26240/60000 (44%)]\tLoss: 9.807303\n",
      "Train Epoch: 4 [26880/60000 (45%)]\tLoss: 8.685884\n",
      "Train Epoch: 4 [27520/60000 (46%)]\tLoss: 2.663198\n",
      "Train Epoch: 4 [28160/60000 (47%)]\tLoss: 14.199425\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 4 [28800/60000 (48%)]\tLoss: 11.116791\n",
      "Train Epoch: 4 [29440/60000 (49%)]\tLoss: 2.402918\n",
      "Train Epoch: 4 [30080/60000 (50%)]\tLoss: 4.401239\n",
      "Train Epoch: 4 [30720/60000 (51%)]\tLoss: 4.655196\n",
      "Train Epoch: 4 [31360/60000 (52%)]\tLoss: 4.891407\n",
      "Train Epoch: 4 [32000/60000 (53%)]\tLoss: 2.961356\n",
      "Train Epoch: 4 [32640/60000 (54%)]\tLoss: 10.901689\n",
      "Train Epoch: 4 [33280/60000 (55%)]\tLoss: 0.554322\n",
      "Train Epoch: 4 [33920/60000 (57%)]\tLoss: 9.157825\n",
      "Train Epoch: 4 [34560/60000 (58%)]\tLoss: 6.211045\n",
      "Train Epoch: 4 [35200/60000 (59%)]\tLoss: 3.544980\n",
      "Train Epoch: 4 [35840/60000 (60%)]\tLoss: 5.057472\n",
      "Train Epoch: 4 [36480/60000 (61%)]\tLoss: 3.552627\n",
      "Train Epoch: 4 [37120/60000 (62%)]\tLoss: 2.418842\n",
      "Train Epoch: 4 [37760/60000 (63%)]\tLoss: 12.234315\n",
      "Train Epoch: 4 [38400/60000 (64%)]\tLoss: 2.502735\n",
      "Train Epoch: 4 [39040/60000 (65%)]\tLoss: 5.953590\n",
      "Train Epoch: 4 [39680/60000 (66%)]\tLoss: 8.780099\n",
      "Train Epoch: 4 [40320/60000 (67%)]\tLoss: 1.174813\n",
      "Train Epoch: 4 [40960/60000 (68%)]\tLoss: 4.357615\n",
      "Train Epoch: 4 [41600/60000 (69%)]\tLoss: 6.912323\n",
      "Train Epoch: 4 [42240/60000 (70%)]\tLoss: 6.688109\n",
      "Train Epoch: 4 [42880/60000 (71%)]\tLoss: 5.775367\n",
      "Train Epoch: 4 [43520/60000 (72%)]\tLoss: 2.361507\n",
      "Train Epoch: 4 [44160/60000 (74%)]\tLoss: 3.172145\n",
      "Train Epoch: 4 [44800/60000 (75%)]\tLoss: 5.093514\n",
      "Train Epoch: 4 [45440/60000 (76%)]\tLoss: 5.098560\n",
      "Train Epoch: 4 [46080/60000 (77%)]\tLoss: 4.200459\n",
      "Train Epoch: 4 [46720/60000 (78%)]\tLoss: 19.048227\n",
      "Train Epoch: 4 [47360/60000 (79%)]\tLoss: 5.187266\n",
      "Train Epoch: 4 [48000/60000 (80%)]\tLoss: 4.273799\n",
      "Train Epoch: 4 [48640/60000 (81%)]\tLoss: 3.563021\n",
      "Train Epoch: 4 [49280/60000 (82%)]\tLoss: 2.276229\n",
      "Train Epoch: 4 [49920/60000 (83%)]\tLoss: 4.273859\n",
      "Train Epoch: 4 [50560/60000 (84%)]\tLoss: 8.572746\n",
      "Train Epoch: 4 [51200/60000 (85%)]\tLoss: 1.500271\n",
      "Train Epoch: 4 [51840/60000 (86%)]\tLoss: 9.243051\n",
      "Train Epoch: 4 [52480/60000 (87%)]\tLoss: 2.367111\n",
      "Train Epoch: 4 [53120/60000 (88%)]\tLoss: 4.907752\n",
      "Train Epoch: 4 [53760/60000 (90%)]\tLoss: 0.582087\n",
      "Train Epoch: 4 [54400/60000 (91%)]\tLoss: 11.228354\n",
      "Train Epoch: 4 [55040/60000 (92%)]\tLoss: 11.071354\n",
      "Train Epoch: 4 [55680/60000 (93%)]\tLoss: 2.255403\n",
      "Train Epoch: 4 [56320/60000 (94%)]\tLoss: 4.642889\n",
      "Train Epoch: 4 [56960/60000 (95%)]\tLoss: 13.126541\n",
      "Train Epoch: 4 [57600/60000 (96%)]\tLoss: 5.112108\n",
      "Train Epoch: 4 [58240/60000 (97%)]\tLoss: 0.478988\n",
      "Train Epoch: 4 [58880/60000 (98%)]\tLoss: 4.067495\n",
      "Train Epoch: 4 [59520/60000 (99%)]\tLoss: 7.656174\n",
      "\n",
      "Test set: Average loss: 0.0544, Accuracy: 9806/10000 (98.06%)\n",
      "\n",
      "Train Epoch: 5 [0/60000 (0%)]\tLoss: 5.114225\n",
      "Train Epoch: 5 [640/60000 (1%)]\tLoss: 15.170694\n",
      "Train Epoch: 5 [1280/60000 (2%)]\tLoss: 6.286937\n",
      "Train Epoch: 5 [1920/60000 (3%)]\tLoss: 9.043308\n",
      "Train Epoch: 5 [2560/60000 (4%)]\tLoss: 3.879147\n",
      "Train Epoch: 5 [3200/60000 (5%)]\tLoss: 8.280998\n",
      "Train Epoch: 5 [3840/60000 (6%)]\tLoss: 7.126341\n",
      "Train Epoch: 5 [4480/60000 (7%)]\tLoss: 2.286620\n",
      "Train Epoch: 5 [5120/60000 (9%)]\tLoss: 8.981131\n",
      "Train Epoch: 5 [5760/60000 (10%)]\tLoss: 4.134356\n",
      "Train Epoch: 5 [6400/60000 (11%)]\tLoss: 8.072508\n",
      "Train Epoch: 5 [7040/60000 (12%)]\tLoss: 1.396859\n",
      "Train Epoch: 5 [7680/60000 (13%)]\tLoss: 1.746360\n",
      "Train Epoch: 5 [8320/60000 (14%)]\tLoss: 0.627814\n",
      "Train Epoch: 5 [8960/60000 (15%)]\tLoss: 3.069103\n",
      "Train Epoch: 5 [9600/60000 (16%)]\tLoss: 3.112091\n",
      "Train Epoch: 5 [10240/60000 (17%)]\tLoss: 9.434679\n",
      "Train Epoch: 5 [10880/60000 (18%)]\tLoss: 3.272834\n",
      "Train Epoch: 5 [11520/60000 (19%)]\tLoss: 1.694075\n",
      "Train Epoch: 5 [12160/60000 (20%)]\tLoss: 4.576614\n",
      "Train Epoch: 5 [12800/60000 (21%)]\tLoss: 7.254579\n",
      "Train Epoch: 5 [13440/60000 (22%)]\tLoss: 11.116033\n",
      "Train Epoch: 5 [14080/60000 (23%)]\tLoss: 19.152424\n",
      "Train Epoch: 5 [14720/60000 (25%)]\tLoss: 1.422675\n",
      "Train Epoch: 5 [15360/60000 (26%)]\tLoss: 4.007272\n",
      "Train Epoch: 5 [16000/60000 (27%)]\tLoss: 0.750134\n",
      "Train Epoch: 5 [16640/60000 (28%)]\tLoss: 3.925351\n",
      "Train Epoch: 5 [17280/60000 (29%)]\tLoss: 21.118919\n",
      "Train Epoch: 5 [17920/60000 (30%)]\tLoss: 12.813771\n",
      "Train Epoch: 5 [18560/60000 (31%)]\tLoss: 3.368471\n",
      "Train Epoch: 5 [19200/60000 (32%)]\tLoss: 2.883131\n",
      "Train Epoch: 5 [19840/60000 (33%)]\tLoss: 4.932166\n",
      "Train Epoch: 5 [20480/60000 (34%)]\tLoss: 1.483761\n",
      "Train Epoch: 5 [21120/60000 (35%)]\tLoss: 2.958440\n",
      "Train Epoch: 5 [21760/60000 (36%)]\tLoss: 9.304618\n",
      "Train Epoch: 5 [22400/60000 (37%)]\tLoss: 2.724267\n",
      "Train Epoch: 5 [23040/60000 (38%)]\tLoss: 0.943876\n",
      "Train Epoch: 5 [23680/60000 (39%)]\tLoss: 6.344690\n",
      "Train Epoch: 5 [24320/60000 (41%)]\tLoss: 8.505790\n",
      "Train Epoch: 5 [24960/60000 (42%)]\tLoss: 12.012214\n",
      "Train Epoch: 5 [25600/60000 (43%)]\tLoss: 7.215436\n",
      "Train Epoch: 5 [26240/60000 (44%)]\tLoss: 13.054042\n",
      "Train Epoch: 5 [26880/60000 (45%)]\tLoss: 3.121171\n",
      "Train Epoch: 5 [27520/60000 (46%)]\tLoss: 6.279863\n",
      "Train Epoch: 5 [28160/60000 (47%)]\tLoss: 3.578970\n",
      "Train Epoch: 5 [28800/60000 (48%)]\tLoss: 18.609404\n",
      "Train Epoch: 5 [29440/60000 (49%)]\tLoss: 4.110584\n",
      "Train Epoch: 5 [30080/60000 (50%)]\tLoss: 11.201428\n",
      "Train Epoch: 5 [30720/60000 (51%)]\tLoss: 0.463449\n",
      "Train Epoch: 5 [31360/60000 (52%)]\tLoss: 1.479442\n",
      "Train Epoch: 5 [32000/60000 (53%)]\tLoss: 3.813271\n",
      "Train Epoch: 5 [32640/60000 (54%)]\tLoss: 11.557169\n",
      "Train Epoch: 5 [33280/60000 (55%)]\tLoss: 10.667287\n",
      "Train Epoch: 5 [33920/60000 (57%)]\tLoss: 9.466030\n",
      "Train Epoch: 5 [34560/60000 (58%)]\tLoss: 3.419337\n",
      "Train Epoch: 5 [35200/60000 (59%)]\tLoss: 12.729682\n",
      "Train Epoch: 5 [35840/60000 (60%)]\tLoss: 5.988071\n",
      "Train Epoch: 5 [36480/60000 (61%)]\tLoss: 3.726662\n",
      "Train Epoch: 5 [37120/60000 (62%)]\tLoss: 4.299050\n",
      "Train Epoch: 5 [37760/60000 (63%)]\tLoss: 4.394792\n",
      "Train Epoch: 5 [38400/60000 (64%)]\tLoss: 14.899055\n",
      "Train Epoch: 5 [39040/60000 (65%)]\tLoss: 2.244035\n",
      "Train Epoch: 5 [39680/60000 (66%)]\tLoss: 6.475419\n",
      "Train Epoch: 5 [40320/60000 (67%)]\tLoss: 1.610528\n",
      "Train Epoch: 5 [40960/60000 (68%)]\tLoss: 17.250280\n",
      "Train Epoch: 5 [41600/60000 (69%)]\tLoss: 8.912785\n",
      "Train Epoch: 5 [42240/60000 (70%)]\tLoss: 2.690377\n",
      "Train Epoch: 5 [42880/60000 (71%)]\tLoss: 19.946255\n",
      "Train Epoch: 5 [43520/60000 (72%)]\tLoss: 0.436165\n",
      "Train Epoch: 5 [44160/60000 (74%)]\tLoss: 9.205739\n",
      "Train Epoch: 5 [44800/60000 (75%)]\tLoss: 5.441026\n",
      "Train Epoch: 5 [45440/60000 (76%)]\tLoss: 4.476599\n",
      "Train Epoch: 5 [46080/60000 (77%)]\tLoss: 6.965637\n",
      "Train Epoch: 5 [46720/60000 (78%)]\tLoss: 4.882964\n",
      "Train Epoch: 5 [47360/60000 (79%)]\tLoss: 5.793066\n",
      "Train Epoch: 5 [48000/60000 (80%)]\tLoss: 6.843711\n",
      "Train Epoch: 5 [48640/60000 (81%)]\tLoss: 0.467783\n",
      "Train Epoch: 5 [49280/60000 (82%)]\tLoss: 1.068542\n",
      "Train Epoch: 5 [49920/60000 (83%)]\tLoss: 8.641273\n",
      "Train Epoch: 5 [50560/60000 (84%)]\tLoss: 7.366841\n",
      "Train Epoch: 5 [51200/60000 (85%)]\tLoss: 6.694343\n",
      "Train Epoch: 5 [51840/60000 (86%)]\tLoss: 7.029500\n",
      "Train Epoch: 5 [52480/60000 (87%)]\tLoss: 13.054803\n",
      "Train Epoch: 5 [53120/60000 (88%)]\tLoss: 8.345400\n",
      "Train Epoch: 5 [53760/60000 (90%)]\tLoss: 14.149642\n",
      "Train Epoch: 5 [54400/60000 (91%)]\tLoss: 2.524837\n",
      "Train Epoch: 5 [55040/60000 (92%)]\tLoss: 2.386958\n",
      "Train Epoch: 5 [55680/60000 (93%)]\tLoss: 1.673861\n",
      "Train Epoch: 5 [56320/60000 (94%)]\tLoss: 2.785835\n",
      "Train Epoch: 5 [56960/60000 (95%)]\tLoss: 1.299375\n",
      "Train Epoch: 5 [57600/60000 (96%)]\tLoss: 16.489639\n",
      "Train Epoch: 5 [58240/60000 (97%)]\tLoss: 9.679877\n",
      "Train Epoch: 5 [58880/60000 (98%)]\tLoss: 8.848861\n",
      "Train Epoch: 5 [59520/60000 (99%)]\tLoss: 1.724942\n",
      "\n",
      "Test set: Average loss: 0.0534, Accuracy: 9835/10000 (98.35%)\n",
      "\n",
      "Train Epoch: 6 [0/60000 (0%)]\tLoss: 6.630224\n",
      "Train Epoch: 6 [640/60000 (1%)]\tLoss: 20.787951\n",
      "Train Epoch: 6 [1280/60000 (2%)]\tLoss: 8.005960\n",
      "Train Epoch: 6 [1920/60000 (3%)]\tLoss: 2.239980\n",
      "Train Epoch: 6 [2560/60000 (4%)]\tLoss: 6.578001\n",
      "Train Epoch: 6 [3200/60000 (5%)]\tLoss: 3.619935\n",
      "Train Epoch: 6 [3840/60000 (6%)]\tLoss: 8.038226\n",
      "Train Epoch: 6 [4480/60000 (7%)]\tLoss: 1.407748\n",
      "Train Epoch: 6 [5120/60000 (9%)]\tLoss: 6.318404\n",
      "Train Epoch: 6 [5760/60000 (10%)]\tLoss: 3.642601\n",
      "Train Epoch: 6 [6400/60000 (11%)]\tLoss: 14.758779\n",
      "Train Epoch: 6 [7040/60000 (12%)]\tLoss: 7.775433\n",
      "Train Epoch: 6 [7680/60000 (13%)]\tLoss: 1.635665\n",
      "Train Epoch: 6 [8320/60000 (14%)]\tLoss: 5.907955\n",
      "Train Epoch: 6 [8960/60000 (15%)]\tLoss: 6.641166\n",
      "Train Epoch: 6 [9600/60000 (16%)]\tLoss: 8.952413\n",
      "Train Epoch: 6 [10240/60000 (17%)]\tLoss: 2.097774\n",
      "Train Epoch: 6 [10880/60000 (18%)]\tLoss: 1.934465\n",
      "Train Epoch: 6 [11520/60000 (19%)]\tLoss: 7.630402\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 6 [12160/60000 (20%)]\tLoss: 10.302482\n",
      "Train Epoch: 6 [12800/60000 (21%)]\tLoss: 6.445203\n",
      "Train Epoch: 6 [13440/60000 (22%)]\tLoss: 1.047648\n",
      "Train Epoch: 6 [14080/60000 (23%)]\tLoss: 13.805943\n",
      "Train Epoch: 6 [14720/60000 (25%)]\tLoss: 6.265696\n",
      "Train Epoch: 6 [15360/60000 (26%)]\tLoss: 4.532309\n",
      "Train Epoch: 6 [16000/60000 (27%)]\tLoss: 9.401494\n",
      "Train Epoch: 6 [16640/60000 (28%)]\tLoss: 3.560211\n",
      "Train Epoch: 6 [17280/60000 (29%)]\tLoss: 6.512724\n",
      "Train Epoch: 6 [17920/60000 (30%)]\tLoss: 6.915971\n",
      "Train Epoch: 6 [18560/60000 (31%)]\tLoss: 6.477233\n",
      "Train Epoch: 6 [19200/60000 (32%)]\tLoss: 1.410539\n",
      "Train Epoch: 6 [19840/60000 (33%)]\tLoss: 13.338208\n",
      "Train Epoch: 6 [20480/60000 (34%)]\tLoss: 3.021915\n",
      "Train Epoch: 6 [21120/60000 (35%)]\tLoss: 13.437016\n",
      "Train Epoch: 6 [21760/60000 (36%)]\tLoss: 2.353795\n",
      "Train Epoch: 6 [22400/60000 (37%)]\tLoss: 1.897857\n",
      "Train Epoch: 6 [23040/60000 (38%)]\tLoss: 7.407558\n",
      "Train Epoch: 6 [23680/60000 (39%)]\tLoss: 9.312606\n",
      "Train Epoch: 6 [24320/60000 (41%)]\tLoss: 7.692962\n",
      "Train Epoch: 6 [24960/60000 (42%)]\tLoss: 1.286190\n",
      "Train Epoch: 6 [25600/60000 (43%)]\tLoss: 0.632134\n",
      "Train Epoch: 6 [26240/60000 (44%)]\tLoss: 1.353311\n",
      "Train Epoch: 6 [26880/60000 (45%)]\tLoss: 8.307919\n",
      "Train Epoch: 6 [27520/60000 (46%)]\tLoss: 5.581457\n",
      "Train Epoch: 6 [28160/60000 (47%)]\tLoss: 8.840015\n",
      "Train Epoch: 6 [28800/60000 (48%)]\tLoss: 6.795627\n",
      "Train Epoch: 6 [29440/60000 (49%)]\tLoss: 3.726153\n",
      "Train Epoch: 6 [30080/60000 (50%)]\tLoss: 1.732226\n",
      "Train Epoch: 6 [30720/60000 (51%)]\tLoss: 11.781883\n",
      "Train Epoch: 6 [31360/60000 (52%)]\tLoss: 4.923038\n",
      "Train Epoch: 6 [32000/60000 (53%)]\tLoss: 6.519748\n",
      "Train Epoch: 6 [32640/60000 (54%)]\tLoss: 5.364420\n",
      "Train Epoch: 6 [33280/60000 (55%)]\tLoss: 7.112742\n",
      "Train Epoch: 6 [33920/60000 (57%)]\tLoss: 2.949376\n",
      "Train Epoch: 6 [34560/60000 (58%)]\tLoss: 3.513264\n",
      "Train Epoch: 6 [35200/60000 (59%)]\tLoss: 1.145744\n",
      "Train Epoch: 6 [35840/60000 (60%)]\tLoss: 1.166405\n",
      "Train Epoch: 6 [36480/60000 (61%)]\tLoss: 1.269870\n",
      "Train Epoch: 6 [37120/60000 (62%)]\tLoss: 2.981927\n",
      "Train Epoch: 6 [37760/60000 (63%)]\tLoss: 2.170358\n",
      "Train Epoch: 6 [38400/60000 (64%)]\tLoss: 9.312818\n",
      "Train Epoch: 6 [39040/60000 (65%)]\tLoss: 15.214147\n",
      "Train Epoch: 6 [39680/60000 (66%)]\tLoss: 11.719201\n",
      "Train Epoch: 6 [40320/60000 (67%)]\tLoss: 13.651773\n",
      "Train Epoch: 6 [40960/60000 (68%)]\tLoss: 0.814875\n",
      "Train Epoch: 6 [41600/60000 (69%)]\tLoss: 11.746740\n",
      "Train Epoch: 6 [42240/60000 (70%)]\tLoss: 2.090991\n",
      "Train Epoch: 6 [42880/60000 (71%)]\tLoss: 3.587035\n",
      "Train Epoch: 6 [43520/60000 (72%)]\tLoss: 10.558620\n",
      "Train Epoch: 6 [44160/60000 (74%)]\tLoss: 2.969190\n",
      "Train Epoch: 6 [44800/60000 (75%)]\tLoss: 3.508309\n",
      "Train Epoch: 6 [45440/60000 (76%)]\tLoss: 5.605505\n",
      "Train Epoch: 6 [46080/60000 (77%)]\tLoss: 1.874039\n",
      "Train Epoch: 6 [46720/60000 (78%)]\tLoss: 0.807255\n",
      "Train Epoch: 6 [47360/60000 (79%)]\tLoss: 7.933768\n",
      "Train Epoch: 6 [48000/60000 (80%)]\tLoss: 2.012141\n",
      "Train Epoch: 6 [48640/60000 (81%)]\tLoss: 8.081161\n",
      "Train Epoch: 6 [49280/60000 (82%)]\tLoss: 6.256367\n",
      "Train Epoch: 6 [49920/60000 (83%)]\tLoss: 3.062191\n",
      "Train Epoch: 6 [50560/60000 (84%)]\tLoss: 2.545498\n",
      "Train Epoch: 6 [51200/60000 (85%)]\tLoss: 2.002692\n",
      "Train Epoch: 6 [51840/60000 (86%)]\tLoss: 9.981467\n",
      "Train Epoch: 6 [52480/60000 (87%)]\tLoss: 5.514098\n",
      "Train Epoch: 6 [53120/60000 (88%)]\tLoss: 15.328537\n",
      "Train Epoch: 6 [53760/60000 (90%)]\tLoss: 4.123604\n",
      "Train Epoch: 6 [54400/60000 (91%)]\tLoss: 6.357366\n",
      "Train Epoch: 6 [55040/60000 (92%)]\tLoss: 1.006755\n",
      "Train Epoch: 6 [55680/60000 (93%)]\tLoss: 6.220472\n",
      "Train Epoch: 6 [56320/60000 (94%)]\tLoss: 8.803873\n",
      "Train Epoch: 6 [56960/60000 (95%)]\tLoss: 3.007529\n",
      "Train Epoch: 6 [57600/60000 (96%)]\tLoss: 24.665640\n",
      "Train Epoch: 6 [58240/60000 (97%)]\tLoss: 7.719577\n",
      "Train Epoch: 6 [58880/60000 (98%)]\tLoss: 4.896190\n",
      "Train Epoch: 6 [59520/60000 (99%)]\tLoss: 2.905648\n",
      "\n",
      "Test set: Average loss: 0.0500, Accuracy: 9841/10000 (98.41%)\n",
      "\n",
      "Train Epoch: 7 [0/60000 (0%)]\tLoss: 4.332672\n",
      "Train Epoch: 7 [640/60000 (1%)]\tLoss: 3.403158\n",
      "Train Epoch: 7 [1280/60000 (2%)]\tLoss: 6.480197\n",
      "Train Epoch: 7 [1920/60000 (3%)]\tLoss: 5.810470\n",
      "Train Epoch: 7 [2560/60000 (4%)]\tLoss: 1.707820\n",
      "Train Epoch: 7 [3200/60000 (5%)]\tLoss: 17.646034\n",
      "Train Epoch: 7 [3840/60000 (6%)]\tLoss: 1.011726\n",
      "Train Epoch: 7 [4480/60000 (7%)]\tLoss: 3.523808\n",
      "Train Epoch: 7 [5120/60000 (9%)]\tLoss: 1.006069\n",
      "Train Epoch: 7 [5760/60000 (10%)]\tLoss: 9.496636\n",
      "Train Epoch: 7 [6400/60000 (11%)]\tLoss: 1.595050\n",
      "Train Epoch: 7 [7040/60000 (12%)]\tLoss: 1.538800\n",
      "Train Epoch: 7 [7680/60000 (13%)]\tLoss: 2.765428\n",
      "Train Epoch: 7 [8320/60000 (14%)]\tLoss: 2.008750\n",
      "Train Epoch: 7 [8960/60000 (15%)]\tLoss: 4.298728\n",
      "Train Epoch: 7 [9600/60000 (16%)]\tLoss: 1.099337\n",
      "Train Epoch: 7 [10240/60000 (17%)]\tLoss: 18.223530\n",
      "Train Epoch: 7 [10880/60000 (18%)]\tLoss: 13.177669\n",
      "Train Epoch: 7 [11520/60000 (19%)]\tLoss: 11.055270\n",
      "Train Epoch: 7 [12160/60000 (20%)]\tLoss: 6.045821\n",
      "Train Epoch: 7 [12800/60000 (21%)]\tLoss: 0.705234\n",
      "Train Epoch: 7 [13440/60000 (22%)]\tLoss: 3.253911\n",
      "Train Epoch: 7 [14080/60000 (23%)]\tLoss: 1.505289\n",
      "Train Epoch: 7 [14720/60000 (25%)]\tLoss: 6.582406\n",
      "Train Epoch: 7 [15360/60000 (26%)]\tLoss: 2.989957\n",
      "Train Epoch: 7 [16000/60000 (27%)]\tLoss: 1.111599\n",
      "Train Epoch: 7 [16640/60000 (28%)]\tLoss: 9.114334\n",
      "Train Epoch: 7 [17280/60000 (29%)]\tLoss: 2.861264\n",
      "Train Epoch: 7 [17920/60000 (30%)]\tLoss: 4.227240\n",
      "Train Epoch: 7 [18560/60000 (31%)]\tLoss: 3.375951\n",
      "Train Epoch: 7 [19200/60000 (32%)]\tLoss: 0.490650\n",
      "Train Epoch: 7 [19840/60000 (33%)]\tLoss: 5.171482\n",
      "Train Epoch: 7 [20480/60000 (34%)]\tLoss: 0.694190\n",
      "Train Epoch: 7 [21120/60000 (35%)]\tLoss: 11.296841\n",
      "Train Epoch: 7 [21760/60000 (36%)]\tLoss: 3.236701\n",
      "Train Epoch: 7 [22400/60000 (37%)]\tLoss: 3.254429\n",
      "Train Epoch: 7 [23040/60000 (38%)]\tLoss: 9.260544\n",
      "Train Epoch: 7 [23680/60000 (39%)]\tLoss: 4.044302\n",
      "Train Epoch: 7 [24320/60000 (41%)]\tLoss: 17.980017\n",
      "Train Epoch: 7 [24960/60000 (42%)]\tLoss: 9.320857\n",
      "Train Epoch: 7 [25600/60000 (43%)]\tLoss: 8.903996\n",
      "Train Epoch: 7 [26240/60000 (44%)]\tLoss: 1.442104\n",
      "Train Epoch: 7 [26880/60000 (45%)]\tLoss: 6.671692\n",
      "Train Epoch: 7 [27520/60000 (46%)]\tLoss: 5.635300\n",
      "Train Epoch: 7 [28160/60000 (47%)]\tLoss: 9.034647\n",
      "Train Epoch: 7 [28800/60000 (48%)]\tLoss: 2.659579\n",
      "Train Epoch: 7 [29440/60000 (49%)]\tLoss: 1.092082\n",
      "Train Epoch: 7 [30080/60000 (50%)]\tLoss: 4.280841\n",
      "Train Epoch: 7 [30720/60000 (51%)]\tLoss: 5.199413\n",
      "Train Epoch: 7 [31360/60000 (52%)]\tLoss: 5.857262\n",
      "Train Epoch: 7 [32000/60000 (53%)]\tLoss: 0.349206\n",
      "Train Epoch: 7 [32640/60000 (54%)]\tLoss: 3.080471\n",
      "Train Epoch: 7 [33280/60000 (55%)]\tLoss: 9.217436\n",
      "Train Epoch: 7 [33920/60000 (57%)]\tLoss: 8.408026\n",
      "Train Epoch: 7 [34560/60000 (58%)]\tLoss: 15.142101\n",
      "Train Epoch: 7 [35200/60000 (59%)]\tLoss: 3.616246\n",
      "Train Epoch: 7 [35840/60000 (60%)]\tLoss: 12.495605\n",
      "Train Epoch: 7 [36480/60000 (61%)]\tLoss: 6.047259\n",
      "Train Epoch: 7 [37120/60000 (62%)]\tLoss: 4.497419\n",
      "Train Epoch: 7 [37760/60000 (63%)]\tLoss: 7.680035\n",
      "Train Epoch: 7 [38400/60000 (64%)]\tLoss: 1.039654\n",
      "Train Epoch: 7 [39040/60000 (65%)]\tLoss: 5.710527\n",
      "Train Epoch: 7 [39680/60000 (66%)]\tLoss: 7.057245\n",
      "Train Epoch: 7 [40320/60000 (67%)]\tLoss: 4.609823\n",
      "Train Epoch: 7 [40960/60000 (68%)]\tLoss: 1.362619\n",
      "Train Epoch: 7 [41600/60000 (69%)]\tLoss: 0.454375\n",
      "Train Epoch: 7 [42240/60000 (70%)]\tLoss: 7.436452\n",
      "Train Epoch: 7 [42880/60000 (71%)]\tLoss: 6.220904\n",
      "Train Epoch: 7 [43520/60000 (72%)]\tLoss: 0.789020\n",
      "Train Epoch: 7 [44160/60000 (74%)]\tLoss: 5.097136\n",
      "Train Epoch: 7 [44800/60000 (75%)]\tLoss: 4.041225\n",
      "Train Epoch: 7 [45440/60000 (76%)]\tLoss: 2.677005\n",
      "Train Epoch: 7 [46080/60000 (77%)]\tLoss: 11.184466\n",
      "Train Epoch: 7 [46720/60000 (78%)]\tLoss: 6.737823\n",
      "Train Epoch: 7 [47360/60000 (79%)]\tLoss: 1.553954\n",
      "Train Epoch: 7 [48000/60000 (80%)]\tLoss: 8.604844\n",
      "Train Epoch: 7 [48640/60000 (81%)]\tLoss: 4.676332\n",
      "Train Epoch: 7 [49280/60000 (82%)]\tLoss: 1.990419\n",
      "Train Epoch: 7 [49920/60000 (83%)]\tLoss: 6.985807\n",
      "Train Epoch: 7 [50560/60000 (84%)]\tLoss: 2.872166\n",
      "Train Epoch: 7 [51200/60000 (85%)]\tLoss: 6.876919\n",
      "Train Epoch: 7 [51840/60000 (86%)]\tLoss: 4.401384\n",
      "Train Epoch: 7 [52480/60000 (87%)]\tLoss: 15.030044\n",
      "Train Epoch: 7 [53120/60000 (88%)]\tLoss: 0.664423\n",
      "Train Epoch: 7 [53760/60000 (90%)]\tLoss: 6.801167\n",
      "Train Epoch: 7 [54400/60000 (91%)]\tLoss: 0.430540\n",
      "Train Epoch: 7 [55040/60000 (92%)]\tLoss: 5.951643\n",
      "Train Epoch: 7 [55680/60000 (93%)]\tLoss: 15.166791\n",
      "Train Epoch: 7 [56320/60000 (94%)]\tLoss: 2.576188\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 7 [56960/60000 (95%)]\tLoss: 8.735505\n",
      "Train Epoch: 7 [57600/60000 (96%)]\tLoss: 2.060519\n",
      "Train Epoch: 7 [58240/60000 (97%)]\tLoss: 4.416695\n",
      "Train Epoch: 7 [58880/60000 (98%)]\tLoss: 0.822052\n",
      "Train Epoch: 7 [59520/60000 (99%)]\tLoss: 1.440630\n",
      "\n",
      "Test set: Average loss: 0.0552, Accuracy: 9831/10000 (98.31%)\n",
      "\n",
      "Train Epoch: 8 [0/60000 (0%)]\tLoss: 3.563952\n",
      "Train Epoch: 8 [640/60000 (1%)]\tLoss: 5.155003\n",
      "Train Epoch: 8 [1280/60000 (2%)]\tLoss: 2.238763\n",
      "Train Epoch: 8 [1920/60000 (3%)]\tLoss: 7.062019\n",
      "Train Epoch: 8 [2560/60000 (4%)]\tLoss: 3.042124\n",
      "Train Epoch: 8 [3200/60000 (5%)]\tLoss: 2.825757\n",
      "Train Epoch: 8 [3840/60000 (6%)]\tLoss: 6.655331\n",
      "Train Epoch: 8 [4480/60000 (7%)]\tLoss: 9.738765\n",
      "Train Epoch: 8 [5120/60000 (9%)]\tLoss: 0.335297\n",
      "Train Epoch: 8 [5760/60000 (10%)]\tLoss: 1.882492\n",
      "Train Epoch: 8 [6400/60000 (11%)]\tLoss: 1.940750\n",
      "Train Epoch: 8 [7040/60000 (12%)]\tLoss: 4.579886\n",
      "Train Epoch: 8 [7680/60000 (13%)]\tLoss: 11.259303\n",
      "Train Epoch: 8 [8320/60000 (14%)]\tLoss: 2.946391\n",
      "Train Epoch: 8 [8960/60000 (15%)]\tLoss: 3.968591\n",
      "Train Epoch: 8 [9600/60000 (16%)]\tLoss: 2.923450\n",
      "Train Epoch: 8 [10240/60000 (17%)]\tLoss: 1.563694\n",
      "Train Epoch: 8 [10880/60000 (18%)]\tLoss: 9.737351\n",
      "Train Epoch: 8 [11520/60000 (19%)]\tLoss: 4.747862\n",
      "Train Epoch: 8 [12160/60000 (20%)]\tLoss: 6.045306\n",
      "Train Epoch: 8 [12800/60000 (21%)]\tLoss: 2.094947\n",
      "Train Epoch: 8 [13440/60000 (22%)]\tLoss: 9.522662\n",
      "Train Epoch: 8 [14080/60000 (23%)]\tLoss: 8.029079\n",
      "Train Epoch: 8 [14720/60000 (25%)]\tLoss: 8.021338\n",
      "Train Epoch: 8 [15360/60000 (26%)]\tLoss: 5.506800\n",
      "Train Epoch: 8 [16000/60000 (27%)]\tLoss: 7.262497\n",
      "Train Epoch: 8 [16640/60000 (28%)]\tLoss: 7.336144\n",
      "Train Epoch: 8 [17280/60000 (29%)]\tLoss: 12.966516\n",
      "Train Epoch: 8 [17920/60000 (30%)]\tLoss: 7.176060\n",
      "Train Epoch: 8 [18560/60000 (31%)]\tLoss: 2.643374\n",
      "Train Epoch: 8 [19200/60000 (32%)]\tLoss: 3.288464\n",
      "Train Epoch: 8 [19840/60000 (33%)]\tLoss: 8.673247\n",
      "Train Epoch: 8 [20480/60000 (34%)]\tLoss: 8.329132\n",
      "Train Epoch: 8 [21120/60000 (35%)]\tLoss: 4.090757\n",
      "Train Epoch: 8 [21760/60000 (36%)]\tLoss: 4.469299\n",
      "Train Epoch: 8 [22400/60000 (37%)]\tLoss: 7.758936\n",
      "Train Epoch: 8 [23040/60000 (38%)]\tLoss: 1.494558\n",
      "Train Epoch: 8 [23680/60000 (39%)]\tLoss: 2.950405\n",
      "Train Epoch: 8 [24320/60000 (41%)]\tLoss: 0.509877\n",
      "Train Epoch: 8 [24960/60000 (42%)]\tLoss: 2.728011\n",
      "Train Epoch: 8 [25600/60000 (43%)]\tLoss: 6.796323\n",
      "Train Epoch: 8 [26240/60000 (44%)]\tLoss: 1.376414\n",
      "Train Epoch: 8 [26880/60000 (45%)]\tLoss: 0.942263\n",
      "Train Epoch: 8 [27520/60000 (46%)]\tLoss: 9.138129\n",
      "Train Epoch: 8 [28160/60000 (47%)]\tLoss: 3.684197\n",
      "Train Epoch: 8 [28800/60000 (48%)]\tLoss: 1.185676\n",
      "Train Epoch: 8 [29440/60000 (49%)]\tLoss: 12.500170\n",
      "Train Epoch: 8 [30080/60000 (50%)]\tLoss: 0.918692\n",
      "Train Epoch: 8 [30720/60000 (51%)]\tLoss: 8.275303\n",
      "Train Epoch: 8 [31360/60000 (52%)]\tLoss: 4.233202\n",
      "Train Epoch: 8 [32000/60000 (53%)]\tLoss: 7.400211\n",
      "Train Epoch: 8 [32640/60000 (54%)]\tLoss: 1.218750\n",
      "Train Epoch: 8 [33280/60000 (55%)]\tLoss: 5.398952\n",
      "Train Epoch: 8 [33920/60000 (57%)]\tLoss: 2.643663\n",
      "Train Epoch: 8 [34560/60000 (58%)]\tLoss: 8.843253\n",
      "Train Epoch: 8 [35200/60000 (59%)]\tLoss: 0.853465\n",
      "Train Epoch: 8 [35840/60000 (60%)]\tLoss: 5.443512\n",
      "Train Epoch: 8 [36480/60000 (61%)]\tLoss: 8.286023\n",
      "Train Epoch: 8 [37120/60000 (62%)]\tLoss: 1.682471\n",
      "Train Epoch: 8 [37760/60000 (63%)]\tLoss: 4.802897\n",
      "Train Epoch: 8 [38400/60000 (64%)]\tLoss: 1.301162\n",
      "Train Epoch: 8 [39040/60000 (65%)]\tLoss: 2.255265\n",
      "Train Epoch: 8 [39680/60000 (66%)]\tLoss: 3.549284\n",
      "Train Epoch: 8 [40320/60000 (67%)]\tLoss: 6.622640\n",
      "Train Epoch: 8 [40960/60000 (68%)]\tLoss: 0.747360\n",
      "Train Epoch: 8 [41600/60000 (69%)]\tLoss: 5.643166\n",
      "Train Epoch: 8 [42240/60000 (70%)]\tLoss: 2.000731\n",
      "Train Epoch: 8 [42880/60000 (71%)]\tLoss: 9.112757\n",
      "Train Epoch: 8 [43520/60000 (72%)]\tLoss: 2.740674\n",
      "Train Epoch: 8 [44160/60000 (74%)]\tLoss: 3.915157\n",
      "Train Epoch: 8 [44800/60000 (75%)]\tLoss: 9.139977\n",
      "Train Epoch: 8 [45440/60000 (76%)]\tLoss: 3.218711\n",
      "Train Epoch: 8 [46080/60000 (77%)]\tLoss: 4.212547\n",
      "Train Epoch: 8 [46720/60000 (78%)]\tLoss: 1.705429\n",
      "Train Epoch: 8 [47360/60000 (79%)]\tLoss: 13.333056\n",
      "Train Epoch: 8 [48000/60000 (80%)]\tLoss: 17.042568\n",
      "Train Epoch: 8 [48640/60000 (81%)]\tLoss: 3.645360\n",
      "Train Epoch: 8 [49280/60000 (82%)]\tLoss: 11.443022\n",
      "Train Epoch: 8 [49920/60000 (83%)]\tLoss: 3.888028\n",
      "Train Epoch: 8 [50560/60000 (84%)]\tLoss: 3.182363\n",
      "Train Epoch: 8 [51200/60000 (85%)]\tLoss: 12.624931\n",
      "Train Epoch: 8 [51840/60000 (86%)]\tLoss: 3.650775\n",
      "Train Epoch: 8 [52480/60000 (87%)]\tLoss: 6.783047\n",
      "Train Epoch: 8 [53120/60000 (88%)]\tLoss: 4.391937\n",
      "Train Epoch: 8 [53760/60000 (90%)]\tLoss: 5.423202\n",
      "Train Epoch: 8 [54400/60000 (91%)]\tLoss: 3.166685\n",
      "Train Epoch: 8 [55040/60000 (92%)]\tLoss: 1.964956\n",
      "Train Epoch: 8 [55680/60000 (93%)]\tLoss: 8.715889\n",
      "Train Epoch: 8 [56320/60000 (94%)]\tLoss: 4.536964\n",
      "Train Epoch: 8 [56960/60000 (95%)]\tLoss: 5.453135\n",
      "Train Epoch: 8 [57600/60000 (96%)]\tLoss: 2.534173\n",
      "Train Epoch: 8 [58240/60000 (97%)]\tLoss: 0.342883\n",
      "Train Epoch: 8 [58880/60000 (98%)]\tLoss: 0.812616\n",
      "Train Epoch: 8 [59520/60000 (99%)]\tLoss: 7.596770\n",
      "\n",
      "Test set: Average loss: 0.0528, Accuracy: 9842/10000 (98.42%)\n",
      "\n",
      "Train Epoch: 9 [0/60000 (0%)]\tLoss: 6.299816\n",
      "Train Epoch: 9 [640/60000 (1%)]\tLoss: 6.907352\n",
      "Train Epoch: 9 [1280/60000 (2%)]\tLoss: 3.600256\n",
      "Train Epoch: 9 [1920/60000 (3%)]\tLoss: 6.660454\n",
      "Train Epoch: 9 [2560/60000 (4%)]\tLoss: 11.351507\n",
      "Train Epoch: 9 [3200/60000 (5%)]\tLoss: 2.773911\n",
      "Train Epoch: 9 [3840/60000 (6%)]\tLoss: 4.367436\n",
      "Train Epoch: 9 [4480/60000 (7%)]\tLoss: 2.032342\n",
      "Train Epoch: 9 [5120/60000 (9%)]\tLoss: 3.777251\n",
      "Train Epoch: 9 [5760/60000 (10%)]\tLoss: 17.067390\n",
      "Train Epoch: 9 [6400/60000 (11%)]\tLoss: 7.372547\n",
      "Train Epoch: 9 [7040/60000 (12%)]\tLoss: 16.462072\n",
      "Train Epoch: 9 [7680/60000 (13%)]\tLoss: 3.625682\n",
      "Train Epoch: 9 [8320/60000 (14%)]\tLoss: 5.310660\n",
      "Train Epoch: 9 [8960/60000 (15%)]\tLoss: 0.962364\n",
      "Train Epoch: 9 [9600/60000 (16%)]\tLoss: 0.368191\n",
      "Train Epoch: 9 [10240/60000 (17%)]\tLoss: 10.755780\n",
      "Train Epoch: 9 [10880/60000 (18%)]\tLoss: 1.793166\n",
      "Train Epoch: 9 [11520/60000 (19%)]\tLoss: 2.456767\n",
      "Train Epoch: 9 [12160/60000 (20%)]\tLoss: 5.134060\n",
      "Train Epoch: 9 [12800/60000 (21%)]\tLoss: 1.750126\n",
      "Train Epoch: 9 [13440/60000 (22%)]\tLoss: 5.367651\n",
      "Train Epoch: 9 [14080/60000 (23%)]\tLoss: 1.163476\n",
      "Train Epoch: 9 [14720/60000 (25%)]\tLoss: 4.598181\n",
      "Train Epoch: 9 [15360/60000 (26%)]\tLoss: 8.274391\n",
      "Train Epoch: 9 [16000/60000 (27%)]\tLoss: 8.173906\n",
      "Train Epoch: 9 [16640/60000 (28%)]\tLoss: 5.058841\n",
      "Train Epoch: 9 [17280/60000 (29%)]\tLoss: 4.430741\n",
      "Train Epoch: 9 [17920/60000 (30%)]\tLoss: 5.342674\n",
      "Train Epoch: 9 [18560/60000 (31%)]\tLoss: 4.361444\n",
      "Train Epoch: 9 [19200/60000 (32%)]\tLoss: 4.571357\n",
      "Train Epoch: 9 [19840/60000 (33%)]\tLoss: 0.736085\n",
      "Train Epoch: 9 [20480/60000 (34%)]\tLoss: 4.031384\n",
      "Train Epoch: 9 [21120/60000 (35%)]\tLoss: 3.569153\n",
      "Train Epoch: 9 [21760/60000 (36%)]\tLoss: 5.078662\n",
      "Train Epoch: 9 [22400/60000 (37%)]\tLoss: 7.778805\n",
      "Train Epoch: 9 [23040/60000 (38%)]\tLoss: 2.583452\n",
      "Train Epoch: 9 [23680/60000 (39%)]\tLoss: 6.220582\n",
      "Train Epoch: 9 [24320/60000 (41%)]\tLoss: 4.795364\n",
      "Train Epoch: 9 [24960/60000 (42%)]\tLoss: 12.972521\n",
      "Train Epoch: 9 [25600/60000 (43%)]\tLoss: 3.887052\n",
      "Train Epoch: 9 [26240/60000 (44%)]\tLoss: 2.735393\n",
      "Train Epoch: 9 [26880/60000 (45%)]\tLoss: 9.341623\n",
      "Train Epoch: 9 [27520/60000 (46%)]\tLoss: 3.290767\n",
      "Train Epoch: 9 [28160/60000 (47%)]\tLoss: 5.803006\n",
      "Train Epoch: 9 [28800/60000 (48%)]\tLoss: 2.812087\n",
      "Train Epoch: 9 [29440/60000 (49%)]\tLoss: 10.696334\n",
      "Train Epoch: 9 [30080/60000 (50%)]\tLoss: 2.395273\n",
      "Train Epoch: 9 [30720/60000 (51%)]\tLoss: 0.924807\n",
      "Train Epoch: 9 [31360/60000 (52%)]\tLoss: 16.396051\n",
      "Train Epoch: 9 [32000/60000 (53%)]\tLoss: 16.035578\n",
      "Train Epoch: 9 [32640/60000 (54%)]\tLoss: 2.964968\n",
      "Train Epoch: 9 [33280/60000 (55%)]\tLoss: 0.567809\n",
      "Train Epoch: 9 [33920/60000 (57%)]\tLoss: 12.047561\n",
      "Train Epoch: 9 [34560/60000 (58%)]\tLoss: 0.363724\n",
      "Train Epoch: 9 [35200/60000 (59%)]\tLoss: 0.670693\n",
      "Train Epoch: 9 [35840/60000 (60%)]\tLoss: 9.722827\n",
      "Train Epoch: 9 [36480/60000 (61%)]\tLoss: 1.352764\n",
      "Train Epoch: 9 [37120/60000 (62%)]\tLoss: 5.063897\n",
      "Train Epoch: 9 [37760/60000 (63%)]\tLoss: 18.174213\n",
      "Train Epoch: 9 [38400/60000 (64%)]\tLoss: 7.493132\n",
      "Train Epoch: 9 [39040/60000 (65%)]\tLoss: 0.802953\n",
      "Train Epoch: 9 [39680/60000 (66%)]\tLoss: 0.523510\n",
      "Train Epoch: 9 [40320/60000 (67%)]\tLoss: 3.561802\n",
      "Train Epoch: 9 [40960/60000 (68%)]\tLoss: 10.903907\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 9 [41600/60000 (69%)]\tLoss: 6.284098\n",
      "Train Epoch: 9 [42240/60000 (70%)]\tLoss: 7.998821\n",
      "Train Epoch: 9 [42880/60000 (71%)]\tLoss: 14.846068\n",
      "Train Epoch: 9 [43520/60000 (72%)]\tLoss: 5.223794\n",
      "Train Epoch: 9 [44160/60000 (74%)]\tLoss: 8.336000\n",
      "Train Epoch: 9 [44800/60000 (75%)]\tLoss: 3.820257\n",
      "Train Epoch: 9 [45440/60000 (76%)]\tLoss: 12.239676\n",
      "Train Epoch: 9 [46080/60000 (77%)]\tLoss: 8.983435\n",
      "Train Epoch: 9 [46720/60000 (78%)]\tLoss: 1.897945\n",
      "Train Epoch: 9 [47360/60000 (79%)]\tLoss: 1.106027\n",
      "Train Epoch: 9 [48000/60000 (80%)]\tLoss: 11.650100\n",
      "Train Epoch: 9 [48640/60000 (81%)]\tLoss: 2.005645\n",
      "Train Epoch: 9 [49280/60000 (82%)]\tLoss: 2.290253\n",
      "Train Epoch: 9 [49920/60000 (83%)]\tLoss: 3.807944\n",
      "Train Epoch: 9 [50560/60000 (84%)]\tLoss: 9.133227\n",
      "Train Epoch: 9 [51200/60000 (85%)]\tLoss: 2.091491\n",
      "Train Epoch: 9 [51840/60000 (86%)]\tLoss: 1.488215\n",
      "Train Epoch: 9 [52480/60000 (87%)]\tLoss: 10.162395\n",
      "Train Epoch: 9 [53120/60000 (88%)]\tLoss: 1.763453\n",
      "Train Epoch: 9 [53760/60000 (90%)]\tLoss: 7.418035\n",
      "Train Epoch: 9 [54400/60000 (91%)]\tLoss: 1.418293\n",
      "Train Epoch: 9 [55040/60000 (92%)]\tLoss: 3.518251\n",
      "Train Epoch: 9 [55680/60000 (93%)]\tLoss: 5.399552\n",
      "Train Epoch: 9 [56320/60000 (94%)]\tLoss: 1.408760\n",
      "Train Epoch: 9 [56960/60000 (95%)]\tLoss: 1.845590\n",
      "Train Epoch: 9 [57600/60000 (96%)]\tLoss: 4.457830\n",
      "Train Epoch: 9 [58240/60000 (97%)]\tLoss: 5.572523\n",
      "Train Epoch: 9 [58880/60000 (98%)]\tLoss: 3.224164\n",
      "Train Epoch: 9 [59520/60000 (99%)]\tLoss: 1.067660\n",
      "\n",
      "Test set: Average loss: 0.0476, Accuracy: 9858/10000 (98.58%)\n",
      "\n",
      "Train Epoch: 10 [0/60000 (0%)]\tLoss: 3.189883\n",
      "Train Epoch: 10 [640/60000 (1%)]\tLoss: 0.836223\n",
      "Train Epoch: 10 [1280/60000 (2%)]\tLoss: 9.152547\n",
      "Train Epoch: 10 [1920/60000 (3%)]\tLoss: 3.182679\n",
      "Train Epoch: 10 [2560/60000 (4%)]\tLoss: 1.494692\n",
      "Train Epoch: 10 [3200/60000 (5%)]\tLoss: 2.478202\n",
      "Train Epoch: 10 [3840/60000 (6%)]\tLoss: 6.070901\n",
      "Train Epoch: 10 [4480/60000 (7%)]\tLoss: 3.426918\n",
      "Train Epoch: 10 [5120/60000 (9%)]\tLoss: 5.933118\n",
      "Train Epoch: 10 [5760/60000 (10%)]\tLoss: 10.106821\n",
      "Train Epoch: 10 [6400/60000 (11%)]\tLoss: 10.747767\n",
      "Train Epoch: 10 [7040/60000 (12%)]\tLoss: 4.511617\n",
      "Train Epoch: 10 [7680/60000 (13%)]\tLoss: 11.486852\n",
      "Train Epoch: 10 [8320/60000 (14%)]\tLoss: 1.078712\n",
      "Train Epoch: 10 [8960/60000 (15%)]\tLoss: 11.409203\n",
      "Train Epoch: 10 [9600/60000 (16%)]\tLoss: 3.687876\n",
      "Train Epoch: 10 [10240/60000 (17%)]\tLoss: 0.529134\n",
      "Train Epoch: 10 [10880/60000 (18%)]\tLoss: 4.930624\n",
      "Train Epoch: 10 [11520/60000 (19%)]\tLoss: 1.219167\n",
      "Train Epoch: 10 [12160/60000 (20%)]\tLoss: 9.000279\n",
      "Train Epoch: 10 [12800/60000 (21%)]\tLoss: 4.446573\n",
      "Train Epoch: 10 [13440/60000 (22%)]\tLoss: 1.125186\n",
      "Train Epoch: 10 [14080/60000 (23%)]\tLoss: 4.197731\n",
      "Train Epoch: 10 [14720/60000 (25%)]\tLoss: 4.674078\n",
      "Train Epoch: 10 [15360/60000 (26%)]\tLoss: 14.364170\n",
      "Train Epoch: 10 [16000/60000 (27%)]\tLoss: 4.401692\n",
      "Train Epoch: 10 [16640/60000 (28%)]\tLoss: 2.568971\n",
      "Train Epoch: 10 [17280/60000 (29%)]\tLoss: 1.709392\n",
      "Train Epoch: 10 [17920/60000 (30%)]\tLoss: 1.695273\n",
      "Train Epoch: 10 [18560/60000 (31%)]\tLoss: 4.276515\n",
      "Train Epoch: 10 [19200/60000 (32%)]\tLoss: 8.406143\n",
      "Train Epoch: 10 [19840/60000 (33%)]\tLoss: 1.393481\n",
      "Train Epoch: 10 [20480/60000 (34%)]\tLoss: 2.328926\n",
      "Train Epoch: 10 [21120/60000 (35%)]\tLoss: 1.179146\n",
      "Train Epoch: 10 [21760/60000 (36%)]\tLoss: 18.213463\n",
      "Train Epoch: 10 [22400/60000 (37%)]\tLoss: 28.134809\n",
      "Train Epoch: 10 [23040/60000 (38%)]\tLoss: 5.610700\n",
      "Train Epoch: 10 [23680/60000 (39%)]\tLoss: 9.046723\n",
      "Train Epoch: 10 [24320/60000 (41%)]\tLoss: 5.086613\n",
      "Train Epoch: 10 [24960/60000 (42%)]\tLoss: 10.278698\n",
      "Train Epoch: 10 [25600/60000 (43%)]\tLoss: 6.327031\n",
      "Train Epoch: 10 [26240/60000 (44%)]\tLoss: 4.921795\n",
      "Train Epoch: 10 [26880/60000 (45%)]\tLoss: 2.608008\n",
      "Train Epoch: 10 [27520/60000 (46%)]\tLoss: 9.652815\n",
      "Train Epoch: 10 [28160/60000 (47%)]\tLoss: 4.122248\n",
      "Train Epoch: 10 [28800/60000 (48%)]\tLoss: 10.243618\n",
      "Train Epoch: 10 [29440/60000 (49%)]\tLoss: 0.773587\n",
      "Train Epoch: 10 [30080/60000 (50%)]\tLoss: 1.965021\n",
      "Train Epoch: 10 [30720/60000 (51%)]\tLoss: 4.634542\n",
      "Train Epoch: 10 [31360/60000 (52%)]\tLoss: 12.785232\n",
      "Train Epoch: 10 [32000/60000 (53%)]\tLoss: 7.444949\n",
      "Train Epoch: 10 [32640/60000 (54%)]\tLoss: 2.399635\n",
      "Train Epoch: 10 [33280/60000 (55%)]\tLoss: 1.768976\n",
      "Train Epoch: 10 [33920/60000 (57%)]\tLoss: 11.278794\n",
      "Train Epoch: 10 [34560/60000 (58%)]\tLoss: 5.509012\n",
      "Train Epoch: 10 [35200/60000 (59%)]\tLoss: 1.690901\n",
      "Train Epoch: 10 [35840/60000 (60%)]\tLoss: 4.509453\n",
      "Train Epoch: 10 [36480/60000 (61%)]\tLoss: 3.220570\n",
      "Train Epoch: 10 [37120/60000 (62%)]\tLoss: 4.345402\n",
      "Train Epoch: 10 [37760/60000 (63%)]\tLoss: 3.304995\n",
      "Train Epoch: 10 [38400/60000 (64%)]\tLoss: 0.640988\n",
      "Train Epoch: 10 [39040/60000 (65%)]\tLoss: 0.882594\n",
      "Train Epoch: 10 [39680/60000 (66%)]\tLoss: 3.994906\n",
      "Train Epoch: 10 [40320/60000 (67%)]\tLoss: 1.970464\n",
      "Train Epoch: 10 [40960/60000 (68%)]\tLoss: 12.330709\n",
      "Train Epoch: 10 [41600/60000 (69%)]\tLoss: 8.882204\n",
      "Train Epoch: 10 [42240/60000 (70%)]\tLoss: 2.998013\n",
      "Train Epoch: 10 [42880/60000 (71%)]\tLoss: 4.760514\n",
      "Train Epoch: 10 [43520/60000 (72%)]\tLoss: 1.368909\n",
      "Train Epoch: 10 [44160/60000 (74%)]\tLoss: 5.985575\n",
      "Train Epoch: 10 [44800/60000 (75%)]\tLoss: 2.713306\n",
      "Train Epoch: 10 [45440/60000 (76%)]\tLoss: 5.131996\n",
      "Train Epoch: 10 [46080/60000 (77%)]\tLoss: 8.018957\n",
      "Train Epoch: 10 [46720/60000 (78%)]\tLoss: 17.563559\n",
      "Train Epoch: 10 [47360/60000 (79%)]\tLoss: 4.790299\n",
      "Train Epoch: 10 [48000/60000 (80%)]\tLoss: 1.066709\n",
      "Train Epoch: 10 [48640/60000 (81%)]\tLoss: 0.915334\n",
      "Train Epoch: 10 [49280/60000 (82%)]\tLoss: 5.555498\n",
      "Train Epoch: 10 [49920/60000 (83%)]\tLoss: 19.835142\n",
      "Train Epoch: 10 [50560/60000 (84%)]\tLoss: 5.899791\n",
      "Train Epoch: 10 [51200/60000 (85%)]\tLoss: 3.401639\n",
      "Train Epoch: 10 [51840/60000 (86%)]\tLoss: 3.151226\n",
      "Train Epoch: 10 [52480/60000 (87%)]\tLoss: 2.130831\n",
      "Train Epoch: 10 [53120/60000 (88%)]\tLoss: 7.961141\n",
      "Train Epoch: 10 [53760/60000 (90%)]\tLoss: 10.690460\n",
      "Train Epoch: 10 [54400/60000 (91%)]\tLoss: 5.678731\n",
      "Train Epoch: 10 [55040/60000 (92%)]\tLoss: 14.535790\n",
      "Train Epoch: 10 [55680/60000 (93%)]\tLoss: 0.634219\n",
      "Train Epoch: 10 [56320/60000 (94%)]\tLoss: 0.568256\n",
      "Train Epoch: 10 [56960/60000 (95%)]\tLoss: 3.415380\n",
      "Train Epoch: 10 [57600/60000 (96%)]\tLoss: 5.356873\n",
      "Train Epoch: 10 [58240/60000 (97%)]\tLoss: 5.134482\n",
      "Train Epoch: 10 [58880/60000 (98%)]\tLoss: 4.017395\n",
      "Train Epoch: 10 [59520/60000 (99%)]\tLoss: 1.607268\n",
      "\n",
      "Test set: Average loss: 0.0473, Accuracy: 9865/10000 (98.65%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 5. Perform training over multiple epochs\n",
    "\n",
    "# start training\n",
    "for epoch in range(1, epochs + 1):\n",
    "    train(epoch)\n",
    "    test(epoch)\n",
    "\n",
    "writer.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-1b31a7f4513a33a5\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-1b31a7f4513a33a5\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          const port = 8008;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%tensorboard --logdir log/MNIST --port=8008\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
