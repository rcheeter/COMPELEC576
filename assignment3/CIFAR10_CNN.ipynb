{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **CIFAR10_CNN**\n",
    "- ELEC 576 HW 2\n",
    "- Robert Heeter\n",
    "- 25 October 2023\n",
    "\n",
    "## **Structure**:\n",
    "- Purpose: Implement a PyTorch image classsification CNN neural network (LeNet5) on the cifar10 dataset\n",
    "\n",
    "1) Set PyTorch metada\n",
    "    - Seed\n",
    "    - TensorFlow output (logging)\n",
    "    - Whether to transfer to gpu (cuda)\n",
    "2) Import data\n",
    "    - Download data\n",
    "    - Create data loaders with batchsize, transforms, scaling\n",
    "3) Define model architecture, loss, and optimizer\n",
    "4) Define test and training loops\n",
    "    - Train:\n",
    "        - Get next batch\n",
    "        - Forward pass through model-\n",
    "        - Calculate loss\n",
    "        - Backward pass from loss (calculates the gradient for each parameter)\n",
    "        - Optimizer: performs weight updates\n",
    "        - Calculate accuracy, other stats\n",
    "    - Test:\n",
    "        - Calculate loss, accuracy, other stats\n",
    "5) Perform training over multiple epochs\n",
    "    - Each epoch:\n",
    "        - Call train loop\n",
    "        - Call test loop\n",
    "\n",
    "## **Acknowledgements**:\n",
    "- Worked with Arielle Sanford\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from datetime import datetime\n",
    "import os\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%load_ext tensorboard\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Set PyTorch metadata\n",
    "\n",
    "batch_size = 128\n",
    "epochs = 10\n",
    "lr = 0.0001\n",
    "try_cuda = True\n",
    "seed = 1000\n",
    "\n",
    "num_classes = 10\n",
    "\n",
    "logging_interval = 10 # how many batches to wait before logging\n",
    "grayscale = True\n",
    "\n",
    "# setting up the logging\n",
    "log_dir = os.path.join(os.getcwd(),'log/CIFAR10', datetime.now().strftime('%b%d_%H-%M-%S'))\n",
    "writer = SummaryWriter(log_dir=log_dir)\n",
    "\n",
    "# deciding whether to send to the cpu or not if available\n",
    "if torch.cuda.is_available() and try_cuda:\n",
    "    cuda = True\n",
    "    torch.cuda.manual_seed(seed)\n",
    "else:\n",
    "    cuda = False\n",
    "    torch.manual_seed(seed)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Import data\n",
    "\n",
    "transform = transforms.Compose([transforms.Grayscale(num_output_channels=1), transforms.ToTensor()]) \n",
    "\n",
    "train_dataset = datasets.CIFAR10('data', train=True, transform=transform, download=True)\n",
    "test_dataset = datasets.CIFAR10('data', train=False, transform=transform, download=True)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=2)\n",
    "\n",
    "def check_data_loader_dim(loader):\n",
    "    # checking the dataset\n",
    "    for images, labels in loader:\n",
    "        print('Image batch dimensions: ', images.shape)\n",
    "        print('Image label dimensions: ', labels.shape)\n",
    "        break\n",
    "\n",
    "check_data_loader_dim(train_loader)\n",
    "check_data_loader_dim(test_loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Defining model architecture, loss, and optimizer\n",
    "\n",
    "# define configuration\n",
    "in_channels = 1 # grayscale images have one channel\n",
    "cuda = False # not using GPU\n",
    "verbose = True\n",
    "\n",
    "layer_1_n_filters = 32\n",
    "layer_2_n_filters = 64\n",
    "fc_1_n_nodes = 1024\n",
    "padding = 2\n",
    "pooling_size = 2\n",
    "kernel_size = 5\n",
    "\n",
    "# calculating the side length of the final activation maps\n",
    "# convolutional layer output width = [(input_width - kernel_size + 2*padding)/stride] + 1\n",
    "conv1_OW = ((layer_1_n_filters - kernel_size + 2*padding)/1) + 1\n",
    "\n",
    "# max-pooling layer output width = [(input_width - pooling_size)/stride] + 1\n",
    "maxpool1_OW = ((conv1_OW - pooling_size)/2) + 1\n",
    "\n",
    "# convolutional layer output width = [(input_width - kernel_size + 2*padding)/stride] + 1\n",
    "conv2_OW = ((maxpool1_OW - kernel_size + 2*padding)/1) + 1\n",
    "\n",
    "# max-pooling layer output width = [(input_width - pooling_size)/stride] + 1\n",
    "maxpool2_OW = ((conv2_OW - 2)/2) + 1\n",
    "\n",
    "final_length = int(maxpool2_OW)\n",
    "\n",
    "if verbose:\n",
    "    print(f\"final_length = {final_length}\")\n",
    "\n",
    "# define architecture\n",
    "class LeNet5(nn.Module):\n",
    "    def __init__(self, num_classes, grayscale=False):\n",
    "        super(LeNet5, self).__init__()\n",
    "\n",
    "        self.grayscale = grayscale\n",
    "        self.num_classes = num_classes\n",
    "        \n",
    "        if self.grayscale:\n",
    "            in_channels = 1\n",
    "        else:\n",
    "            in_channels = 3\n",
    "  \n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, layer_1_n_filters, kernel_size=5, padding=2), # 32 filters with 5x5 kernel \n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Conv2d(layer_1_n_filters, layer_2_n_filters, kernel_size=5, padding=2), # 64 filters with 5x5 kernel\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        )\n",
    "    \n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(final_length*final_length*layer_2_n_filters*in_channels, fc_1_n_nodes),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(fc_1_n_nodes, num_classes),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x) # send input through convolutional layers\n",
    "        x = x.view(x.size(0), -1) # reshaping\n",
    "        x = self.classifier(x) # send input through MLP layers\n",
    "\n",
    "        logits = x # unnormalized\n",
    "        probas = F.softmax(logits, dim=1) # normalized\n",
    "\n",
    "        return logits, probas\n",
    "    \n",
    "model = LeNet5(num_classes=num_classes, grayscale=True)\n",
    "print(model)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "# optimizer = optim.SGD(model.parameters(), lr=lr)\n",
    "\n",
    "print(optimizer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Define test and training loops\n",
    "\n",
    "def train(epoch):\n",
    "    model.train()\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    total_loss = 0.0\n",
    "    correct = 0\n",
    "\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        if cuda:\n",
    "            data, target = data.cuda(), target.cuda()\n",
    "        optimizer.zero_grad()\n",
    "        logits, probas = model(data) # forward pass\n",
    "        loss = criterion(logits, target)\n",
    "        loss.backward() # backward pass\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        # calculate training accuracy\n",
    "        _, predicted = probas.max(1)\n",
    "        correct += predicted.eq(target).sum().item()\n",
    "        if batch_idx % logging_interval == 0:\n",
    "            print(f'Train Epoch: {epoch} [{batch_idx * len(data)}/{len(train_loader.dataset)} ({100. * batch_idx / len(train_loader):.0f}%)]\\tLoss: {loss.item()}')\n",
    "\n",
    "    # calculate and log training metrics\n",
    "    train_loss = total_loss / len(train_loader.dataset)\n",
    "    train_accuracy = 100. * correct / len(train_loader.dataset)\n",
    "    print(f\"Epoch {epoch}: Training Loss: {train_loss:.4f}, Accuracy: {train_accuracy:.2f}%\")\n",
    "        \n",
    "    # add to TensorBoard\n",
    "    writer.add_scalar('Loss/Train', train_loss, epoch)\n",
    "    writer.add_scalar('Accuracy/Train', train_accuracy, epoch)\n",
    "\n",
    "def test(epoch):\n",
    "    model.eval()\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            if cuda:\n",
    "                data, target = data.cuda(), target.cuda()\n",
    "            logits, probas = model(data)\n",
    "            test_loss += criterion(logits, target).item()\n",
    "            _, predicted = probas.max(1)\n",
    "            correct += predicted.eq(target).sum().item()\n",
    "\n",
    "    # calculate and log testing metrics\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    test_accuracy = 100. * correct / len(test_loader.dataset)\n",
    "    print(f\"Test Loss: {test_loss:.4f}, Accuracy: {test_accuracy:.2f}%\")\n",
    "    \n",
    "    # add to TensorBoard\n",
    "    writer.add_scalar('Accuracy/Test', test_accuracy, epoch)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Perform training over multiple epochs\n",
    "\n",
    "# start training\n",
    "for epoch in range(1, epochs + 1):\n",
    "    train(epoch)\n",
    "    test(epoch)\n",
    "\n",
    "writer.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%tensorboard --logdir log/CIFAR10 --port=8008\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display weight filters\n",
    "\n",
    "# access the first convolutional layer\n",
    "first_conv_layer = model.features[0]\n",
    "\n",
    "# get the weight data from the layer\n",
    "filters = first_conv_layer.weight.data\n",
    "\n",
    "# normalize the weights to visualize them\n",
    "filters = filters - filters.min()\n",
    "filters = filters / filters.max()\n",
    "\n",
    "# plot and visualize the filters\n",
    "fig, axes = plt.subplots(4, 8, figsize=(12, 6))\n",
    "for i, ax in enumerate(axes.ravel()):\n",
    "    ax.imshow(filters[i].cpu().numpy()[0], cmap='gray')\n",
    "    ax.axis('off')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display activation statistics\n",
    "\n",
    "model.eval()  # Set the model to evaluation mode\n",
    "\n",
    "conv_layer = model.features[0]  # Assuming you want statistics for the first convolutional layer\n",
    "activation_stats = []  # To store statistics\n",
    "\n",
    "with torch.no_grad():\n",
    "    for data, _ in test_loader:\n",
    "        if cuda:\n",
    "            data = data.cuda()\n",
    "\n",
    "        # pass the test data through the first convolutional layer\n",
    "        activations = conv_layer(data)\n",
    "\n",
    "        # calculate statistics (mean, standard deviation)\n",
    "        mean = activations.mean().item()\n",
    "        std = activations.std().item()\n",
    "\n",
    "        activation_stats.append((mean, std))\n",
    "\n",
    "activation_stats = torch.tensor(activation_stats)\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(activation_stats[:, 0].numpy())\n",
    "plt.title(\"Mean Activation\")\n",
    "plt.xlabel(\"Test Images\")\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(activation_stats[:, 1].numpy())\n",
    "plt.title(\"Stdev Activation\")\n",
    "plt.xlabel(\"Test Images\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
