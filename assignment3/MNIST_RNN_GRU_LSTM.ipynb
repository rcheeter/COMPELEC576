{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **MNIST_RNN**\n",
    "- ELEC 576 HW 2\n",
    "- Robert Heeter\n",
    "- 25 October 2023\n",
    "\n",
    "## **Structure**:\n",
    "- Adjustments:\n",
    "    - Number of nodes in the hidden layer\n",
    "    - Learning rate\n",
    "    - Number of iterations\n",
    "    - Optimizer\n",
    "1) Set PyTorch metadata\n",
    "    - Seed\n",
    "    - TensorFlow output\n",
    "    - Whether to transfer to gpu (cuda)\n",
    "2) Import data\n",
    "    - Download data\n",
    "    - Create data loaders with batchsize, transforms, scaling\n",
    "3) Define model architecture, loss, and optimizer\n",
    "4) Define test and training loops\n",
    "    - Train:\n",
    "        - Get next batch\n",
    "        - Forward pass through model-\n",
    "        - Calculate loss\n",
    "        - Backward pass from loss (calculates the gradient for each parameter)\n",
    "        - Optimizer: performs weight updates\n",
    "5) Perform training over multiple epochs\n",
    "    - Each epoch:\n",
    "        - Call train loop\n",
    "        - Call test loop\n",
    "\n",
    "## **Acknowledgements**:\n",
    "- Worked with Arielle Sanford\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from datetime import datetime\n",
    "import os\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%load_ext tensorboard\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Set PyTorch metadata\n",
    "\n",
    "batch_size = 64\n",
    "test_batch_size = 1000\n",
    "epochs = 10\n",
    "lr = 0.001\n",
    "try_cuda = True\n",
    "seed = 1000\n",
    "logging_interval = 10 # how many batches to wait before logging\n",
    "logging_dir = None\n",
    "\n",
    "# setting up the logging\n",
    "log_dir = os.path.join(os.getcwd(),'log/MNIST', datetime.now().strftime('%b%d_%H-%M-%S'))\n",
    "writer = SummaryWriter(log_dir=log_dir)\n",
    "\n",
    "# deciding whether to send to the cpu or not if available\n",
    "if torch.cuda.is_available() and try_cuda:\n",
    "    cuda = True\n",
    "    torch.cuda.mnaual_seed(seed)\n",
    "else:\n",
    "    cuda = False\n",
    "    torch.manual_seed(seed)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Import data\n",
    "\n",
    "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))])\n",
    "\n",
    "train_dataset = datasets.MNIST('data', train=True, transform=transform, download=True)\n",
    "test_dataset = datasets.MNIST('data', train=False, transform=transform, download=True)\n",
    "\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(dataset=test_dataset, batch_size=test_batch_size, shuffle=False)\n",
    "\n",
    "# example of data\n",
    "print(train_dataset.train_data.size())\n",
    "print(train_dataset.train_labels.size())\n",
    "\n",
    "plt.imshow(train_dataset.train_data[0].numpy(), cmap='gray')\n",
    "plt.title('%i' % train_dataset.train_labels[0])\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Defining model architecture, loss, and optimizer\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self, model_type, hidden_size):\n",
    "        super(Net, self).__init__()\n",
    "        \n",
    "        self.model_type = model_type\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        if self.model_type == 'RNN':\n",
    "            # define RNN layer\n",
    "            self.network = nn.RNN(\n",
    "                input_size=28, # input size (number of features in each time step)\n",
    "                hidden_size=self.hidden_size, # number of hidden units\n",
    "                num_layers=2, # number of RNN layers\n",
    "                batch_first=True # input and output tensors are (batch, time_step, input_size)\n",
    "            )\n",
    "            \n",
    "        elif self.model_type == 'GRU':\n",
    "            # define GRU layer\n",
    "            self.network = nn.GRU(\n",
    "                input_size=28, # input size (number of features in each time step)\n",
    "                hidden_size=self.hidden_size, # number of hidden units\n",
    "                num_layers=2, # number of RNN layers\n",
    "                batch_first=True # input and output tensors are (batch, time_step, input_size)\n",
    "            )\n",
    "\n",
    "        elif self.model_type == 'LSTM':\n",
    "            # define LSTM layer\n",
    "            self.network = nn.LSTM(\n",
    "                input_size=28, # input size (number of features in each time step)\n",
    "                hidden_size=self.hidden_size, # number of hidden units\n",
    "                num_layers=2, # number of RNN layers\n",
    "                batch_first=True # input and output tensors are (batch, time_step, input_size)\n",
    "            )\n",
    "\n",
    "        # define output layer\n",
    "        self.out = nn.Linear(self.hidden_size, 10) # output size = 10\n",
    "\n",
    "    def forward(self, x):\n",
    "        n_out, hidden = self.network(x, None)\n",
    "        return self.out(n_out[:, -1, :])\n",
    "\n",
    "model = Net(model_type='LSTM', hidden_size=30)\n",
    "print(model)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "# optimizer = optim.SGD(model.parameters(), lr=lr)\n",
    "print(optimizer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Define test and training loops\n",
    "\n",
    "def train(epoch):\n",
    "    model.train()\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    \n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        if cuda:\n",
    "            data, target = data.cuda(), target.cuda()\n",
    "\n",
    "        data = data.view(-1, 28, 28)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data) # forward pass\n",
    "        loss = criterion(output, target)\n",
    "        loss.backward() # backward pass\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        # calculate training accuracy\n",
    "        _, pred = output.max(1)\n",
    "        correct += pred.eq(target).sum().item()\n",
    "   \n",
    "        if batch_idx % logging_interval == 0:\n",
    "            print(f'Train Epoch: {epoch} [{batch_idx * len(data)}/{len(train_loader.dataset)} ({100. * batch_idx / len(train_loader):.0f}%)]\\tLoss: {loss.item()}')\n",
    "    \n",
    "    # calculate and log training metrics\n",
    "    train_loss = total_loss / len(train_loader.dataset)\n",
    "    train_accuracy = 100. * correct / len(train_loader.dataset)\n",
    "    print(f\"Epoch {epoch}: Training Loss: {train_loss:.4f}, Accuracy: {train_accuracy:.2f}%\")\n",
    "    \n",
    "    # add to TensorBoard\n",
    "    writer.add_scalar('Loss/Train', train_loss, epoch)\n",
    "    writer.add_scalar('Accuracy/Train', train_accuracy, epoch)\n",
    "\n",
    "def test(epoch):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    \n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            if cuda:\n",
    "                data, target = data.cuda(), target.cuda()\n",
    "\n",
    "            data = data.view(-1, 28, 28)\n",
    "            output = model(data)\n",
    "\n",
    "            test_loss += criterion(output, target,).item() # sum up batch loss (later, averaged over all test samples)\n",
    "            pred = output.data.max(1, keepdim=True)[1] # get index of max\n",
    "            correct += pred.eq(target.data.view_as(pred)).cpu().sum()\n",
    "\n",
    "    # calculate and log testing metrics\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    test_accuracy = 100. * correct / len(test_loader.dataset)\n",
    "    print(f\"Test Loss: {test_loss:.4f}, Accuracy: {test_accuracy:.2f}%\")\n",
    "    \n",
    "    # add to TensorBoard\n",
    "    writer.add_scalar('Accuracy/Test', test_accuracy, epoch)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Perform training over multiple epochs\n",
    "\n",
    "# start training\n",
    "for epoch in range(1, epochs + 1):\n",
    "    train(epoch)\n",
    "    test(epoch)\n",
    "\n",
    "writer.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%tensorboard --logdir log/MNIST --port=8008\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
