{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **ELEC 576-HW1-P2**\n",
    "- Robert Heeter\n",
    "- 4 October 2023\n",
    "\n",
    "## **Structure**:\n",
    "1) Set PyTorch metadata\n",
    "    - Seed\n",
    "    - TensorFlow output\n",
    "    - Whether to transfer to gpu (cuda)\n",
    "2) Import data\n",
    "    - Download data\n",
    "    - Create data loaders with batchsize, transforms, scaling\n",
    "3) Define model architecture, loss, and optimizer\n",
    "4) Define test and training loops\n",
    "    - Train:\n",
    "        - Get next batch\n",
    "        - Forward pass through model-\n",
    "        - Calculate loss\n",
    "        - Backward pass from loss (calculates the gradient for each parameter)\n",
    "        - Optimizer: performs weight updates\n",
    "5) Perform training over multiple epochs\n",
    "    - Each epoch:\n",
    "        - Call train loop\n",
    "        - Call test loop\n",
    "\n",
    "## **Acknowledgements**:\n",
    "- https://colab.research.google.com/drive/1i9KpbQyFU4zfq8zLLns8a2Kd8PRMGsaZ\n",
    "- https://github.com/motokimura/pytorch_tensorboard/blob/master/main.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from datetime import datetime\n",
    "import os\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Set PyTorch metadata\n",
    "\n",
    "batch_size = 64\n",
    "test_batch_size = 1000\n",
    "epochs = 10\n",
    "lr = 0.01\n",
    "try_cuda = True\n",
    "seed = 1000\n",
    "logging_interval = 10 # how many batches to wait before logging\n",
    "logging_dir = None\n",
    "\n",
    "writer = SummaryWriter()\n",
    "\n",
    "# setting up the logging\n",
    "[inset-code: set up logging]\n",
    "\n",
    "# deciding whether to send to the cpu or not if available\n",
    "if torch.cuda.is_available() and try_cuda:\n",
    "    cuda = True\n",
    "    torch.cuda.mnaual_seed(seed)\n",
    "else:\n",
    "    cuda = False\n",
    "    torch.manual_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Import data\n",
    "\n",
    "transform=transforms.Compose([transforms.ToTensor(),\n",
    "                              transforms.Normalize((0.01307,), (0.3081,))])\n",
    "\n",
    "train_loader = [inset-code]\n",
    "test_loader = [inset-code]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Defining model architecture, loss, and optimizer\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = [inset-code]\n",
    "        self.conv2 = [inset-code]\n",
    "        self.conv2_drop = [inset-code]\n",
    "        self.fc1 = [inset-code]\n",
    "        self.fc2 = [inset-code]\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = [inset-code]\n",
    "        x = [inset-code]\n",
    "        x = [inset-code]\n",
    "        x = [inset-code]\n",
    "        x = [inset-code]\n",
    "        x = [inset-code]\n",
    "        x = F.softmax(x,dim=1)\n",
    "        return x\n",
    "\n",
    "model = [inset-code: instantiate model]\n",
    "\n",
    "optimizer = [inset-code: USE AN ADAM OPTIMIZER]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Define test and training loops\n",
    "\n",
    "eps=1e-13\n",
    "\n",
    "def train(epoch):\n",
    "    model.train()\n",
    "\n",
    "    # criterion = nn.CrossEntropyLoss()\n",
    "    criterion = [inset-code]\n",
    "\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        if cuda:\n",
    "            data, target = data.cuda(), target.cuda()\n",
    "\n",
    "        optimizer.[inset-code]\n",
    "        output = [inset-code]\n",
    "        loss = criterion(torch.log(output+eps), target) # = sum_k(-t_k * log(y_k))\n",
    "        loss[inset-code]\n",
    "        optimizer[inset-code]\n",
    "\n",
    "        if batch_idx % logging_interval == 0:\n",
    "            [inset-code: print and log the performance]\n",
    "\n",
    "    # log model parameters to TensorBoard at every epoch\n",
    "    for name, param in model.named_parameters():\n",
    "        layer, attr = os.path.splitext(name)\n",
    "        attr = attr[1:]\n",
    "        writer.add_histogram(\n",
    "            f'{layer}/{attr}',\n",
    "            param.clone().cpu().data.numpy(),\n",
    "            n_iter)\n",
    "\n",
    "def test(epoch):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    \n",
    "    # criterion = nn.CrossEntropyLoss()\n",
    "    # criterion = nn.CrossEntropyLoss(size_average = False)\n",
    "    criterion = nn.NLLLoss(size_average = False)\n",
    "\n",
    "    for data, target in test_loader:\n",
    "        if cuda:\n",
    "            data, target = data.cuda(), target.cuda()\n",
    "\n",
    "        output = model(data)\n",
    "\n",
    "        test_loss += criterion(torch.log(output+eps), target,).item() # sum up batch loss (later, averaged over all test samples)\n",
    "        pred = [inset-code] # get the index of the max log-probability\n",
    "        correct += [inset-code]\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    test_accuracy = 100. * correct / len(test_loader.dataset)\n",
    "    [inset-code: print the performance]\n",
    "\n",
    "    # log test/loss and test/accuracy to TensorBoard at every epoch\n",
    "    n_iter = epoch * len(train_loader)\n",
    "    [inset-code: log the performance]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Perform training over multiple epochs\n",
    "\n",
    "[inset-code: running test and training over epoch]\n",
    "\n",
    "writer.close()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
